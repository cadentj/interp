{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from nnsight.envoy import Envoy\n",
    "from nnsight.util import WrapperModule\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from transformers.utils import fx as tfx\n",
    "import torch.fx as fx\n",
    "from torch.fx import replace_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     shape_stuff \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/nnsight/src/nnsight/models/LanguageModel.py:140\u001b[0m, in \u001b[0;36mLanguageModel.__init__\u001b[0;34m(self, tokenizer, automodel, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model: PreTrainedModel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomodel \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    135\u001b[0m     automodel\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(automodel, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(modeling_auto, automodel)\n\u001b[1;32m    138\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nnsight/src/nnsight/models/NNsightModel.py:75\u001b[0m, in \u001b[0;36mNNsight.__init__\u001b[0;34m(self, model_key, dispatch, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatched:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Dispatch ._model on initialization vs lazy dispatching.\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envoy \u001b[38;5;241m=\u001b[39m Envoy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model)\n",
      "File \u001b[0;32m~/nnsight/src/nnsight/models/NNsightModel.py:269\u001b[0m, in \u001b[0;36mNNsight.dispatch_model\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch ._model to have real parameters  using .load(...).\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDispatching `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envoy \u001b[38;5;241m=\u001b[39m Envoy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/nnsight/src/nnsight/models/LanguageModel.py:161\u001b[0m, in \u001b[0;36mLanguageModel._load\u001b[0;34m(self, repo_id, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m, WrapperModule())\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m--> 161\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28msetattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m, WrapperModule())\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:3766\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3761\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   3762\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has some weights that should be kept in higher precision, you need to upgrade \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`accelerate` to properly deal with them (`pip install --upgrade accelerate`).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3764\u001b[0m     )\n\u001b[1;32m   3765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 3766\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_low_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3771\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3772\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3774\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m get_max_memory(max_memory)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py:849\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[0;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[1;32m    848\u001b[0m user_not_set_max_memory \u001b[38;5;241m=\u001b[39m max_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n\u001b[1;32m    852\u001b[0m     num_devices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m max_memory \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(d)\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m max_memory[d] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py:720\u001b[0m, in \u001b[0;36mget_max_memory\u001b[0;34m(max_memory)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()):\n\u001b[0;32m--> 720\u001b[0m             _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m         max_memory \u001b[38;5;241m=\u001b[39m {i: torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmem_get_info(i)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())}\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# allocate everything in the mps device as the RAM is shared\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"gpt2\", device_map=\"auto\", dispatch=True)\n",
    "\n",
    "with model.trace(\"a\"):\n",
    "    shape_stuff = model.transformer.h[3].attn.output.shape\n",
    "\n",
    "attention = model._model.transformer.h[3].attn\n",
    "attention_envoy = model._envoy.transformer.h[3].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Conv1D()\n",
       "  (c_proj): Conv1D()\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_wrapper): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_module = WrapperModule()\n",
    "wrapper_name = 'output_wrapper'\n",
    "\n",
    "setattr(attention, wrapper_name, wrapper_module)\n",
    "\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom backend called with FX graph:\n",
      "opcode       name       target     args            kwargs\n",
      "-----------  ---------  ---------  --------------  -----------------------\n",
      "placeholder  s0         s0         ()              {}\n",
      "placeholder  l_tensor_  L_tensor_  ()              {}\n",
      "call_method  to         to         (l_tensor_, 0)  {'non_blocking': False}\n",
      "output       output     output     ((to,),)        {}\n",
      "Custom backend called with FX graph:\n",
      "opcode         name             target                                                    args                                    kwargs\n",
      "-------------  ---------------  --------------------------------------------------------  --------------------------------------  --------\n",
      "placeholder    l_stack0_0_0_    L_stack0_0_0_                                             ()                                      {}\n",
      "placeholder    l_module_nf      L_module_nf                                               ()                                      {}\n",
      "placeholder    l_module_bias    L_module_bias                                             ()                                      {}\n",
      "placeholder    l_module_weight  L_module_weight                                           ()                                      {}\n",
      "call_method    size             size                                                      (l_stack0_0_0_,)                        {}\n",
      "call_method    size_1           size                                                      (l_stack0_0_0_, -1)                     {}\n",
      "call_method    view             view                                                      (l_stack0_0_0_, -1, size_1)             {}\n",
      "call_function  x                <built-in method addmm of type object at 0x7f5c81fa78a0>  (l_module_bias, view, l_module_weight)  {}\n",
      "call_method    output           view                                                      (x, (1, 1, l_module_nf))                {}\n",
      "output         output_1         output                                                    ((output,),)                            {}\n",
      "Custom backend called with FX graph:\n",
      "opcode         name               target                       args                                                kwargs\n",
      "-------------  -----------------  ---------------------------  --------------------------------------------------  ----------\n",
      "placeholder    l_stack0_          L_stack0_                    ()                                                  {}\n",
      "placeholder    l_self_split_size  L_self_split_size            ()                                                  {}\n",
      "placeholder    l_self_num_heads   L_self_num_heads             ()                                                  {}\n",
      "placeholder    l_self_head_dim    L_self_head_dim              ()                                                  {}\n",
      "call_method    split              split                        (l_stack0_, l_self_split_size)                      {'dim': 2}\n",
      "call_function  query              <built-in function getitem>  (split, 0)                                          {}\n",
      "call_function  key                <built-in function getitem>  (split, 1)                                          {}\n",
      "call_function  value              <built-in function getitem>  (split, 2)                                          {}\n",
      "call_method    size               size                         (query,)                                            {}\n",
      "call_method    tensor             view                         (query, (1, 1, l_self_num_heads, l_self_head_dim))  {}\n",
      "call_method    query_1            permute                      (tensor, 0, 2, 1, 3)                                {}\n",
      "call_method    size_1             size                         (key,)                                              {}\n",
      "call_method    tensor_1           view                         (key, (1, 1, l_self_num_heads, l_self_head_dim))    {}\n",
      "call_method    key_1              permute                      (tensor_1, 0, 2, 1, 3)                              {}\n",
      "call_method    size_2             size                         (value,)                                            {}\n",
      "call_method    tensor_2           view                         (value, (1, 1, l_self_num_heads, l_self_head_dim))  {}\n",
      "call_method    value_1            permute                      (tensor_2, 0, 2, 1, 3)                              {}\n",
      "output         output             output                       ((query_1, key_1, value_1),)                        {}\n",
      "Custom backend called with FX graph:\n",
      "opcode         name            target                                                     args                                                                                                        kwargs\n",
      "-------------  --------------  ---------------------------------------------------------  ----------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------\n",
      "placeholder    s0              s0                                                         ()                                                                                                          {}\n",
      "placeholder    s1              s1                                                         ()                                                                                                          {}\n",
      "placeholder    s2              s2                                                         ()                                                                                                          {}\n",
      "placeholder    l_key_          L_key_                                                     ()                                                                                                          {}\n",
      "placeholder    l_query_        L_query_                                                   ()                                                                                                          {}\n",
      "placeholder    s3              s3                                                         ()                                                                                                          {}\n",
      "placeholder    l_value_        L_value_                                                   ()                                                                                                          {}\n",
      "placeholder    s4              s4                                                         ()                                                                                                          {}\n",
      "placeholder    l_self_bias     L_self_bias                                                ()                                                                                                          {}\n",
      "call_method    transpose       transpose                                                  (l_key_, -1, -2)                                                                                            {}\n",
      "call_function  attn_weights    <built-in method matmul of type object at 0x7f5c81fa78a0>  (l_query_, transpose)                                                                                       {}\n",
      "call_method    size            size                                                       (l_value_, -1)                                                                                              {}\n",
      "call_function  pow_1           <built-in function pow>                                    (size, 0.5)                                                                                                 {}\n",
      "call_function  full            <built-in method full of type object at 0x7f5c81fa78a0>    ([], pow_1)                                                                                                 {'dtype': torch.float32, 'device': device(type='cuda', index=0)}\n",
      "call_function  attn_weights_1  <built-in function truediv>                                (attn_weights, full)                                                                                        {}\n",
      "call_function  causal_mask     <built-in function getitem>                                (l_self_bias, (slice(None, None, None), slice(None, None, None), slice(0, 1, None), slice(None, 1, None)))  {}\n",
      "call_function  mask_value      <built-in method full of type object at 0x7f5c81fa78a0>    ([], -3.4028234663852886e+38)                                                                               {'dtype': torch.float32, 'device': device(type='cuda', index=0)}\n",
      "call_method    to              to                                                         (attn_weights_1, torch.float32)                                                                             {}\n",
      "call_function  attn_weights_2  <built-in method where of type object at 0x7f5c81fa78a0>   (causal_mask, to, mask_value)                                                                               {}\n",
      "call_function  attn_weights_3  <function softmax at 0x7f5b9f2cd620>                       (attn_weights_2,)                                                                                           {'dim': -1}\n",
      "call_method    attn_weights_4  type                                                       (attn_weights_3, torch.float32)                                                                             {}\n",
      "output         output          output                                                     ((attn_weights_4,),)                                                                                        {}\n",
      "Custom backend called with FX graph:\n",
      "opcode         name          target                                                     args                      kwargs\n",
      "-------------  ------------  ---------------------------------------------------------  ------------------------  --------\n",
      "placeholder    s0            s0                                                         ()                        {}\n",
      "placeholder    attn_weights  L_stack0_                                                  ()                        {}\n",
      "placeholder    s1            s1                                                         ()                        {}\n",
      "placeholder    s2            s2                                                         ()                        {}\n",
      "placeholder    l_value_      L_value_                                                   ()                        {}\n",
      "call_function  attn_output   <built-in method matmul of type object at 0x7f5c81fa78a0>  (attn_weights, l_value_)  {}\n",
      "output         output        output                                                     ((attn_output,),)         {}\n",
      "Custom backend called with FX graph:\n",
      "opcode         name              target                   args                                 kwargs\n",
      "-------------  ----------------  -----------------------  -----------------------------------  --------\n",
      "placeholder    s0                s0                       ()                                   {}\n",
      "placeholder    s1                s1                       ()                                   {}\n",
      "placeholder    attn_output       L_stack0_0_              ()                                   {}\n",
      "placeholder    l_self_num_heads  L_self_num_heads         ()                                   {}\n",
      "placeholder    l_self_head_dim   L_self_head_dim          ()                                   {}\n",
      "call_method    permute           permute                  (attn_output, 0, 2, 1, 3)            {}\n",
      "call_method    tensor            contiguous               (permute,)                           {}\n",
      "call_method    size              size                     (tensor,)                            {}\n",
      "call_function  mul               <built-in function mul>  (l_self_num_heads, l_self_head_dim)  {}\n",
      "call_method    attn_output_1     view                     (tensor, (1, 1, mul))                {}\n",
      "output         output            output                   ((attn_output_1,),)                  {}\n",
      "Custom backend called with FX graph:\n",
      "opcode         name             target                                                    args                                    kwargs\n",
      "-------------  ---------------  --------------------------------------------------------  --------------------------------------  --------\n",
      "placeholder    l_stack0_0_0_    L_stack0_0_0_                                             ()                                      {}\n",
      "placeholder    l_module_nf      L_module_nf                                               ()                                      {}\n",
      "placeholder    l_module_bias    L_module_bias                                             ()                                      {}\n",
      "placeholder    l_module_weight  L_module_weight                                           ()                                      {}\n",
      "call_method    size             size                                                      (l_stack0_0_0_,)                        {}\n",
      "call_method    size_1           size                                                      (l_stack0_0_0_, -1)                     {}\n",
      "call_method    view             view                                                      (l_stack0_0_0_, -1, size_1)             {}\n",
      "call_function  x                <built-in method addmm of type object at 0x7f5c81fa78a0>  (l_module_bias, view, l_module_weight)  {}\n",
      "call_method    output           view                                                      (x, (1, 1, l_module_nf))                {}\n",
      "output         output_1         output                                                    ((output,),)                            {}\n"
     ]
    }
   ],
   "source": [
    "def custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"Custom backend called with FX graph:\")\n",
    "\n",
    "\n",
    "    # for node in gm.graph.nodes:    \n",
    "        \n",
    "        # if node.op == 'output' and node.name == \"output\":\n",
    "            \n",
    "        #     with gm.graph.inserting_before(node):\n",
    "        #         wrapper_args = node.args\n",
    "        #         wrapper_kwargs = node.kwargs\n",
    "        #         wrapper_node = gm.graph.call_module(wrapper_name, args=wrapper_args, kwargs=wrapper_kwargs)\n",
    "\n",
    "        #         node.replace_all_uses_with(wrapper_node)\n",
    "\n",
    "    gm.recompile()\n",
    "\n",
    "    gm.graph.print_tabular()\n",
    "    \n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "\n",
    "opt_model = torch.compile(attention, backend=custom_backend, dynamic=True)\n",
    "gm = opt_model(attention_envoy._fake_inputs[0][0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
