{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7ff816bce810>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from nnsight.envoy import Envoy\n",
    "from nnsight.util import WrapperModule\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from transformers.utils import fx as tfx\n",
    "import torch.fx as fx\n",
    "from torch.fx import replace_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"gpt2\", device_map=\"auto\", dispatch=True)\n",
    "\n",
    "with model.trace(\"a\"):\n",
    "    sample_input = model.transformer.h[3].mlp.input.save()\n",
    "\n",
    "mlp = model._model.transformer.h[3].mlp\n",
    "mlp_envoy = model._envoy.transformer.h[3].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2MLP(\n",
       "  (c_fc): Conv1D()\n",
       "  (c_proj): Conv1D()\n",
       "  (act): NewGELUActivation()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_wrapper): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_module = WrapperModule()\n",
    "wrapper_name = 'output_wrapper'\n",
    "\n",
    "setattr(mlp, wrapper_name, wrapper_module)\n",
    "\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_fc = model._model.transformer.h[3].mlp.act\n",
    "c_fc_envoy = model._envoy.transformer.h[3].mlp.act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom backend called with FX graph:\n",
      "opcode         name      target                                                   args                       kwargs\n",
      "-------------  --------  -------------------------------------------------------  -------------------------  --------\n",
      "placeholder    l_input_  L_input_                                                 ()                         {}\n",
      "call_function  mul       <built-in function mul>                                  (0.5, l_input_)            {}\n",
      "call_function  pow_1     <built-in method pow of type object at 0x7ff7efe858a0>   (l_input_, 3.0)            {}\n",
      "call_function  mul_1     <built-in function mul>                                  (0.044715, pow_1)          {}\n",
      "call_function  add       <built-in function add>                                  (l_input_, mul_1)          {}\n",
      "call_function  mul_2     <built-in function mul>                                  (0.7978845608028654, add)  {}\n",
      "call_function  tanh      <built-in method tanh of type object at 0x7ff7efe858a0>  (mul_2,)                   {}\n",
      "call_function  add_1     <built-in function add>                                  (1.0, tanh)                {}\n",
      "call_function  mul_3     <built-in function mul>                                  (mul, add_1)               {}\n",
      "output         output    output                                                   ((mul_3,),)                {}\n"
     ]
    }
   ],
   "source": [
    "def custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"Custom backend called with FX graph:\")\n",
    "\n",
    "    gm.graph.print_tabular()\n",
    "    \n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(c_fc, backend=custom_backend)\n",
    "gm = opt_model(c_fc_envoy._fake_inputs[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Custom Backend:\n",
      "graph():\n",
      "    %l_tensor_ : torch._subclasses.fake_tensor.FakeTensor [num_users=1] = placeholder[target=L_tensor_]\n",
      "    %to : [num_users=1] = call_method[target=to](args = (%l_tensor_, 0), kwargs = {non_blocking: False})\n",
      "    return (to,)\n",
      "My Custom Backend:\n",
      "graph():\n",
      "    %l_stack0_0_0_ : torch._subclasses.fake_tensor.FakeTensor [num_users=1] = placeholder[target=L_stack0_0_0_]\n",
      "    %l_module_bias : torch.nn.parameter.Parameter [num_users=1] = placeholder[target=L_module_bias]\n",
      "    %l_module_weight : torch.nn.parameter.Parameter [num_users=1] = placeholder[target=L_module_weight]\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%l_stack0_0_0_, -1, 768), kwargs = {})\n",
      "    %x : [num_users=1] = call_function[target=torch.addmm](args = (%l_module_bias, %view, %l_module_weight), kwargs = {})\n",
      "    %output : [num_users=1] = call_method[target=view](args = (%x, (1, 1, 3072)), kwargs = {})\n",
      "    return (output,)\n",
      "My Custom Backend:\n",
      "graph():\n",
      "    %l_input_ : torch._subclasses.fake_tensor.FakeTensor [num_users=3] = placeholder[target=L_input_]\n",
      "    %mul : [num_users=1] = call_function[target=operator.mul](args = (0.5, %l_input_), kwargs = {})\n",
      "    %pow_1 : [num_users=1] = call_function[target=torch.pow](args = (%l_input_, 3.0), kwargs = {})\n",
      "    %mul_1 : [num_users=1] = call_function[target=operator.mul](args = (0.044715, %pow_1), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%l_input_, %mul_1), kwargs = {})\n",
      "    %mul_2 : [num_users=1] = call_function[target=operator.mul](args = (0.7978845608028654, %add), kwargs = {})\n",
      "    %tanh : [num_users=1] = call_function[target=torch.tanh](args = (%mul_2,), kwargs = {})\n",
      "    %add_1 : [num_users=1] = call_function[target=operator.add](args = (1.0, %tanh), kwargs = {})\n",
      "    %mul_3 : [num_users=1] = call_function[target=operator.mul](args = (%mul, %add_1), kwargs = {})\n",
      "    return (mul_3,)\n",
      "My Custom Backend:\n",
      "graph():\n",
      "    %l_stack0_0_0_ : torch._subclasses.fake_tensor.FakeTensor [num_users=3] = placeholder[target=L_stack0_0_0_]\n",
      "    %l_module_nf : torch.SymInt [num_users=1] = placeholder[target=L_module_nf]\n",
      "    %l_module_bias : torch.nn.parameter.Parameter [num_users=1] = placeholder[target=L_module_bias]\n",
      "    %l_module_weight : torch.nn.parameter.Parameter [num_users=1] = placeholder[target=L_module_weight]\n",
      "    %size : [num_users=0] = call_method[target=size](args = (%l_stack0_0_0_,), kwargs = {})\n",
      "    %size_1 : [num_users=1] = call_method[target=size](args = (%l_stack0_0_0_, -1), kwargs = {})\n",
      "    %view : [num_users=1] = call_method[target=view](args = (%l_stack0_0_0_, -1, %size_1), kwargs = {})\n",
      "    %x : [num_users=1] = call_function[target=torch.addmm](args = (%l_module_bias, %view, %l_module_weight), kwargs = {})\n",
      "    %output : [num_users=1] = call_method[target=view](args = (%x, (1, 1, %l_module_nf)), kwargs = {})\n",
      "    return (output,)\n"
     ]
    }
   ],
   "source": [
    "def my_custom_backend(gm, example_inputs):\n",
    "    print(\"My Custom Backend:\")\n",
    "    print(gm.graph)\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(mlp, backend=my_custom_backend)\n",
    "gm = opt_model(mlp_envoy._fake_inputs[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Custom Backend:\n",
      "opcode         name                target                                args                      kwargs\n",
      "-------------  ------------------  ------------------------------------  ------------------------  --------\n",
      "placeholder    l_x_                L_x_                                  ()                        {}\n",
      "call_module    x                   L__self___layer1                      (l_x_,)                   {}\n",
      "call_module    x_1                 L__self___layer2                      (x,)                      {}\n",
      "call_module    x_2                 L__self___layer3                      (x_1,)                    {}\n",
      "call_module    l__self___nested_0  L__self___nested_0                    (x_2,)                    {}\n",
      "call_module    x_3                 L__self___nested_1                    (l__self___nested_0,)     {}\n",
      "call_module    x_4                 L__self___wrapped_mod                 (x_3,)                    {}\n",
      "call_function  x_5                 <function dropout at 0x7ff71a344a40>  (x_4, 0.1, False, False)  {}\n",
      "output         output              output                                ((x_5,),)                 {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4337, -0.4469, -0.4084, -0.3884],\n",
       "          [-0.3759, -0.4469, -0.3876, -0.4294],\n",
       "          [-0.4178, -0.4482, -0.4013, -0.4244],\n",
       "          [-0.4022, -0.4329, -0.4290, -0.4147]]]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class WrappedBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mod = nn.BatchNorm2d(1)\n",
    "    def forward(self, x):\n",
    "        return self.mod(x)\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Conv2d(1, 1, 1)\n",
    "        self.layer2 = nn.BatchNorm2d(1)\n",
    "        self.layer3 = nn.Conv2d(1, 1, 1)\n",
    "        self.nested = nn.Sequential(\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Conv2d(1, 1, 1),\n",
    "        )\n",
    "        self.wrapped = WrappedBatchNorm()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.nested(x)\n",
    "        x = self.wrapped(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "def my_custom_backend(gm, example_inputs):\n",
    "    print(\"My Custom Backend:\")\n",
    "    gm.graph.print_tabular()\n",
    "    return gm.forward\n",
    "\n",
    "mod = M()\n",
    "mod.eval()\n",
    "mod = torch.compile(mod, backend=my_custom_backend)\n",
    "mod(torch.randn(1,1,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math \n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n",
    "\n",
    "    Basically works like a linear layer but the weights are transposed.\n",
    "\n",
    "    Args:\n",
    "        nf (`int`): The number of output features.\n",
    "        nx (`int`): The number of input features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nf, nx):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        self.weight = nn.Parameter(torch.empty(nx, nf))\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "        nn.init.normal_(self.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(size_out)\n",
    "        return x\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self, intermediate_size, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        self.c_fc = Conv1D(intermediate_size, embed_dim)\n",
    "        self.c_proj = Conv1D(embed_dim, intermediate_size)\n",
    "        self.act = NewGELUActivation()\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        self.layer3 = nn.Conv2d(1, 1, 1)\n",
    "\n",
    "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # var = torch.randn(1,1,1,1)\n",
    "        # hidden_states = self.layer3(var)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<function my_custom_backend at 0x7ff6eee74d60>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(my_custom_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Custom Backend:\n",
      "opcode         name                     target                                                    args                                                      kwargs\n",
      "-------------  -----------------------  --------------------------------------------------------  --------------------------------------------------------  --------\n",
      "placeholder    l_hidden_states_         L_hidden_states_                                          ()                                                        {}\n",
      "get_attr       l__self___c_fc_bias      L__self___c_fc_bias                                       ()                                                        {}\n",
      "call_method    view                     view                                                      (l_hidden_states_, -1, 768)                               {}\n",
      "get_attr       l__self___c_fc_weight    L__self___c_fc_weight                                     ()                                                        {}\n",
      "call_function  x                        <built-in method addmm of type object at 0x7ff7efe858a0>  (l__self___c_fc_bias, view, l__self___c_fc_weight)        {}\n",
      "call_method    hidden_states            view                                                      (x, (1, 1, 64))                                           {}\n",
      "call_function  mul                      <built-in function mul>                                   (0.5, hidden_states)                                      {}\n",
      "call_function  pow_1                    <built-in method pow of type object at 0x7ff7efe858a0>    (hidden_states, 3.0)                                      {}\n",
      "call_function  mul_1                    <built-in function mul>                                   (0.044715, pow_1)                                         {}\n",
      "call_function  add                      <built-in function add>                                   (hidden_states, mul_1)                                    {}\n",
      "call_function  mul_2                    <built-in function mul>                                   (0.7978845608028654, add)                                 {}\n",
      "call_function  tanh                     <built-in method tanh of type object at 0x7ff7efe858a0>   (mul_2,)                                                  {}\n",
      "call_function  add_1                    <built-in function add>                                   (1.0, tanh)                                               {}\n",
      "call_function  hidden_states_1          <built-in function mul>                                   (mul, add_1)                                              {}\n",
      "get_attr       l__self___c_proj_bias    L__self___c_proj_bias                                     ()                                                        {}\n",
      "call_method    view_2                   view                                                      (hidden_states_1, -1, 64)                                 {}\n",
      "get_attr       l__self___c_proj_weight  L__self___c_proj_weight                                   ()                                                        {}\n",
      "call_function  x_2                      <built-in method addmm of type object at 0x7ff7efe858a0>  (l__self___c_proj_bias, view_2, l__self___c_proj_weight)  {}\n",
      "call_method    hidden_states_2          view                                                      (x_2, (1, 1, 768))                                        {}\n",
      "call_function  hidden_states_3          <function dropout at 0x7ff71a344a40>                      (hidden_states_2, 0.1, True, False)                       {}\n",
      "output         output                   output                                                    ((hidden_states_3,),)                                     {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1716e-02,  2.4820e-02, -0.0000e+00,  1.1286e-02, -9.4275e-04,\n",
       "           1.2199e-03,  2.3217e-02,  3.4676e-02, -9.3017e-03,  2.7910e-02,\n",
       "           0.0000e+00, -3.1078e-02, -2.8279e-03, -1.5981e-02, -7.8414e-03,\n",
       "           5.2942e-03,  2.0214e-02,  8.8420e-03,  1.5737e-02, -3.8330e-03,\n",
       "           1.0344e-02, -1.1160e-02,  1.5577e-02,  1.6235e-02,  2.9588e-02,\n",
       "           2.6649e-02, -7.3694e-03,  2.3291e-02,  2.2564e-02,  7.5243e-03,\n",
       "          -1.0763e-02, -4.1770e-03,  1.3237e-02, -3.4895e-03, -3.4665e-03,\n",
       "           1.8950e-02, -2.2814e-02, -6.8608e-03, -2.8047e-02,  7.2334e-03,\n",
       "          -1.4093e-02, -2.0041e-02, -3.2132e-02, -1.0461e-02, -0.0000e+00,\n",
       "           6.1334e-03, -3.6673e-03,  3.5334e-02,  3.0345e-02, -0.0000e+00,\n",
       "           4.9096e-02, -5.5823e-03, -3.6563e-03,  7.1525e-03,  1.8869e-03,\n",
       "          -1.6341e-02, -4.4186e-02,  2.3387e-02,  2.7945e-02,  1.1505e-02,\n",
       "           1.9473e-03,  1.0681e-02, -1.2614e-02, -7.9247e-03, -0.0000e+00,\n",
       "           8.4066e-04,  4.1705e-02,  5.2352e-03, -5.6634e-03, -1.9478e-02,\n",
       "          -5.3758e-03,  3.4008e-02,  1.2891e-02,  1.0601e-02,  2.6312e-02,\n",
       "          -1.0883e-02, -1.6328e-02,  2.2742e-02, -7.9369e-03,  1.6556e-03,\n",
       "           2.1361e-03,  0.0000e+00,  8.7733e-03,  2.9803e-03, -5.5447e-03,\n",
       "          -2.8478e-02,  1.2145e-03, -0.0000e+00, -1.4642e-02,  2.3749e-04,\n",
       "          -3.3983e-03,  1.7386e-02, -0.0000e+00,  1.0077e-02, -2.0142e-02,\n",
       "           1.4023e-05, -1.2188e-02, -1.9949e-04,  0.0000e+00, -2.8303e-02,\n",
       "          -0.0000e+00,  0.0000e+00, -7.0215e-03,  1.3759e-02, -1.5363e-02,\n",
       "          -4.1987e-03, -0.0000e+00, -4.8596e-03, -1.4489e-02,  7.2796e-03,\n",
       "           6.4167e-03,  4.5566e-03, -1.6294e-02,  1.6533e-02, -1.3530e-02,\n",
       "           3.1750e-03, -0.0000e+00,  3.3291e-02, -7.0136e-03, -4.3218e-03,\n",
       "           6.4804e-03, -1.9709e-02,  1.7937e-02, -0.0000e+00,  9.3118e-04,\n",
       "          -8.7223e-03,  1.9084e-02, -9.9158e-05, -2.6013e-02, -2.2107e-02,\n",
       "           1.2489e-02,  0.0000e+00,  1.9526e-02,  1.8242e-02, -2.6165e-02,\n",
       "          -7.9221e-03, -1.0981e-02, -1.2289e-03,  1.1769e-02,  0.0000e+00,\n",
       "           1.1415e-02, -3.1718e-02, -4.1674e-02,  5.9665e-03,  0.0000e+00,\n",
       "           1.8359e-02,  2.8332e-03,  1.9933e-02,  1.9055e-02, -1.6460e-02,\n",
       "          -2.2356e-02, -1.7105e-02,  1.2096e-02,  2.2405e-03, -9.1984e-03,\n",
       "          -9.5820e-03, -3.6540e-02,  1.7321e-03, -8.7023e-03, -2.8064e-02,\n",
       "           5.9159e-03,  1.4519e-02,  1.3710e-03,  3.1771e-02,  1.2670e-02,\n",
       "          -1.3594e-02, -1.6069e-02, -8.6523e-03,  3.5911e-02,  2.8415e-03,\n",
       "          -4.4303e-03,  2.8382e-02, -2.6780e-03, -5.0785e-04,  8.6946e-03,\n",
       "          -1.6790e-02,  1.4553e-02,  7.2188e-03, -0.0000e+00,  3.1233e-03,\n",
       "           4.5254e-03, -0.0000e+00, -1.9294e-02,  0.0000e+00, -7.6483e-03,\n",
       "          -1.7758e-02, -5.2177e-05, -5.5796e-02, -2.5658e-02,  2.6501e-04,\n",
       "           6.3326e-03, -1.2223e-02, -3.6460e-03, -3.4898e-02,  2.7160e-03,\n",
       "           1.2155e-02,  1.2538e-02,  1.1110e-02, -0.0000e+00,  1.8644e-02,\n",
       "          -5.4469e-02, -4.1532e-03,  1.5683e-02, -1.6215e-02, -6.4552e-03,\n",
       "          -2.0337e-02, -7.4374e-03,  1.0253e-03,  1.0010e-02, -2.0973e-02,\n",
       "          -1.5478e-02, -0.0000e+00,  1.1434e-02, -5.3741e-03, -1.6599e-02,\n",
       "          -6.3434e-03,  1.4818e-02,  8.1785e-03, -0.0000e+00,  3.5574e-03,\n",
       "           3.9753e-02, -0.0000e+00, -7.3792e-03, -3.2759e-02, -8.5647e-03,\n",
       "          -0.0000e+00,  8.7454e-03,  2.4731e-04, -2.3080e-02, -1.0386e-02,\n",
       "          -1.0550e-02,  2.8341e-02,  1.0403e-02, -3.1399e-02, -1.3887e-02,\n",
       "          -5.1946e-03,  1.6448e-02,  1.6573e-02,  1.5253e-04, -2.1531e-03,\n",
       "           9.2972e-03,  1.8021e-02,  3.2170e-02, -2.2707e-02,  2.4205e-02,\n",
       "           3.2008e-02,  6.6518e-03, -1.5498e-02,  6.6374e-03,  1.7055e-02,\n",
       "           3.9813e-02, -2.7054e-02,  0.0000e+00, -0.0000e+00, -1.2634e-02,\n",
       "           2.7757e-03, -2.2100e-02,  2.2278e-02, -1.2310e-03, -6.6144e-02,\n",
       "           0.0000e+00,  1.1317e-03, -5.9878e-05, -1.4652e-02, -0.0000e+00,\n",
       "          -2.4678e-02,  3.1369e-02, -2.6517e-02, -1.0416e-02, -6.8928e-03,\n",
       "          -1.6037e-03,  1.1658e-02, -1.5787e-02,  2.3383e-02, -1.6870e-02,\n",
       "           1.2241e-02,  8.6100e-03,  2.6653e-02,  0.0000e+00, -3.0482e-03,\n",
       "           6.2299e-03,  1.7324e-02,  6.8713e-03,  6.7858e-03,  6.7933e-04,\n",
       "           0.0000e+00, -1.9233e-02,  1.3331e-02, -1.5373e-02,  9.6678e-03,\n",
       "           2.4911e-03, -2.1435e-02,  1.0985e-02, -2.0665e-02,  8.6341e-03,\n",
       "          -1.6173e-02,  1.1927e-02, -1.0954e-02,  2.9758e-04, -5.4507e-03,\n",
       "          -8.8460e-04, -3.7795e-03, -9.7302e-03, -2.6333e-02,  1.6793e-02,\n",
       "           3.7070e-03, -2.2274e-03, -2.4897e-02,  4.2794e-02,  9.7853e-03,\n",
       "          -0.0000e+00, -2.2284e-02,  4.5730e-04, -0.0000e+00, -5.6135e-04,\n",
       "          -1.7567e-02,  6.6053e-04,  1.9964e-03,  0.0000e+00,  8.9034e-03,\n",
       "           3.0191e-04, -2.9844e-02, -5.6955e-03, -1.2528e-02, -2.2571e-03,\n",
       "          -1.5154e-02, -3.3615e-02, -3.4762e-04,  3.1345e-02,  3.1785e-02,\n",
       "          -1.3272e-02,  3.0836e-02,  1.7169e-02, -1.6100e-02,  2.0922e-02,\n",
       "          -4.4266e-02, -3.0997e-02, -0.0000e+00, -0.0000e+00, -2.8027e-02,\n",
       "           2.1586e-02, -2.9341e-02,  2.4901e-03,  3.1934e-03, -4.1890e-03,\n",
       "          -1.3870e-02, -1.7905e-02, -7.2079e-03, -1.7567e-02,  2.2121e-02,\n",
       "           2.7100e-02, -0.0000e+00,  4.6867e-02, -2.0937e-02, -1.3104e-02,\n",
       "           1.9262e-02, -9.2453e-03, -1.8844e-02,  8.7340e-03, -7.3522e-03,\n",
       "           4.4820e-03,  2.6211e-02,  2.5786e-04, -1.3184e-02, -1.2412e-02,\n",
       "           1.9983e-02, -8.1005e-04,  0.0000e+00, -2.0395e-02, -3.4812e-02,\n",
       "           5.0794e-03,  0.0000e+00, -2.4388e-03, -1.4450e-03, -6.6932e-04,\n",
       "          -2.1584e-02, -1.7220e-02,  7.6574e-04, -2.8751e-03,  1.4794e-02,\n",
       "           0.0000e+00, -7.8580e-03, -1.3090e-02, -1.7256e-02, -5.9169e-03,\n",
       "           2.2847e-03, -1.0894e-02,  5.6330e-04, -3.8767e-03, -1.8164e-02,\n",
       "           3.9378e-03,  2.1476e-02,  1.2543e-03,  1.6159e-02, -1.6142e-02,\n",
       "           0.0000e+00, -0.0000e+00, -2.5564e-02, -8.4631e-03, -1.7740e-02,\n",
       "           2.2360e-02,  2.9517e-02, -8.0176e-03, -0.0000e+00, -2.5335e-02,\n",
       "           1.0675e-02, -3.4830e-03, -1.9324e-02, -1.7209e-02,  1.1002e-02,\n",
       "           6.4890e-04,  2.3297e-02, -4.7072e-03,  5.5080e-04,  1.6408e-02,\n",
       "           1.7723e-02,  4.8521e-02,  4.0304e-02, -9.2897e-03, -1.2983e-02,\n",
       "           5.8334e-04,  2.2807e-03,  1.7324e-02,  0.0000e+00,  1.4763e-02,\n",
       "          -2.3564e-03, -9.0889e-03, -2.1713e-02,  1.2306e-02, -1.9192e-04,\n",
       "           2.3260e-02,  2.5901e-03,  9.7167e-03,  1.5696e-02,  6.6562e-03,\n",
       "          -1.5031e-02,  2.9424e-02,  6.4637e-03,  4.4957e-03,  0.0000e+00,\n",
       "           1.5343e-02, -1.5677e-02, -2.5552e-02,  0.0000e+00,  2.3631e-02,\n",
       "          -0.0000e+00, -0.0000e+00, -5.1028e-03, -0.0000e+00,  1.1879e-02,\n",
       "          -3.0309e-03,  2.3563e-02,  5.8735e-02,  6.9523e-03,  2.8302e-02,\n",
       "          -0.0000e+00, -6.1189e-03,  1.8873e-02,  7.4570e-03,  3.0834e-02,\n",
       "           4.8289e-02,  1.4820e-02,  2.5105e-02,  2.1513e-02,  7.4968e-05,\n",
       "          -1.9901e-02,  5.7450e-03,  2.5049e-02,  8.5980e-03, -1.8346e-02,\n",
       "          -2.0779e-03,  4.3764e-04, -1.6566e-02,  4.7826e-04,  2.1657e-02,\n",
       "           3.0187e-03,  2.7944e-02,  0.0000e+00, -8.6528e-03,  2.1300e-02,\n",
       "          -7.6231e-03, -2.0319e-02,  1.7982e-02, -4.4462e-03,  2.6129e-02,\n",
       "          -1.3577e-02, -7.9845e-03,  6.4371e-02,  0.0000e+00, -1.6903e-02,\n",
       "           1.3636e-02,  1.5809e-02, -6.1977e-03, -5.0883e-03,  3.0829e-02,\n",
       "           1.3795e-03,  3.8818e-02, -2.1124e-02,  2.7908e-02,  3.6025e-03,\n",
       "           3.2296e-02, -4.6255e-03, -1.1466e-02,  1.2677e-02,  5.8111e-02,\n",
       "           3.1687e-02, -2.7245e-02,  3.6883e-04, -1.8745e-02, -2.2462e-03,\n",
       "           0.0000e+00,  0.0000e+00, -2.7577e-03, -2.5568e-02,  1.8314e-02,\n",
       "           1.4450e-02,  1.1431e-02, -5.4674e-03, -7.8353e-04,  3.2589e-02,\n",
       "           0.0000e+00,  1.4362e-02,  1.6036e-02,  2.7347e-02,  3.9902e-02,\n",
       "          -1.9091e-02, -7.2375e-03,  1.7216e-02, -1.1448e-02, -6.1024e-02,\n",
       "          -1.4868e-02, -2.0866e-02,  7.2368e-03,  1.5766e-02, -2.1651e-02,\n",
       "           0.0000e+00, -1.0369e-02, -1.3175e-03,  5.0474e-03,  2.8168e-02,\n",
       "           1.8452e-02, -2.0969e-02, -1.7138e-02,  2.5759e-02,  1.8540e-02,\n",
       "           4.2099e-03, -1.3850e-02,  0.0000e+00,  1.4782e-02,  1.5389e-03,\n",
       "           4.5156e-03, -1.8664e-02, -3.6341e-03,  1.1554e-02,  2.7384e-02,\n",
       "           9.1379e-03, -1.4611e-02, -1.6056e-02,  3.9923e-02,  1.2179e-03,\n",
       "           1.1074e-02,  5.4202e-04,  1.4075e-03, -3.9663e-03, -1.0607e-02,\n",
       "          -9.1366e-03,  3.0273e-02, -2.0400e-02, -2.1021e-02,  4.3672e-02,\n",
       "           0.0000e+00, -1.3546e-02,  2.3948e-03, -3.1489e-03,  1.6530e-02,\n",
       "           1.0854e-03,  3.4039e-02,  1.2347e-02, -4.2709e-04,  7.0140e-03,\n",
       "          -1.8123e-02, -0.0000e+00, -2.7082e-03,  2.5129e-02, -1.7295e-02,\n",
       "           4.8094e-03, -1.2508e-02,  6.4453e-03,  7.1259e-03, -3.2135e-02,\n",
       "          -8.7166e-03,  2.4788e-02, -0.0000e+00, -1.7914e-02, -6.2352e-03,\n",
       "          -1.8323e-03, -2.8190e-02,  1.0437e-04, -5.8419e-04, -2.3729e-03,\n",
       "          -2.5033e-03,  1.2799e-02,  3.2490e-02,  1.5922e-02, -3.6610e-02,\n",
       "          -6.8309e-03,  3.1799e-02,  1.6193e-02,  1.6051e-02, -0.0000e+00,\n",
       "           4.3329e-02, -1.4785e-02,  1.6978e-02, -3.8188e-02, -4.6097e-03,\n",
       "          -0.0000e+00,  4.1473e-02,  2.5369e-02,  1.6434e-02, -1.4411e-02,\n",
       "          -1.9074e-03,  3.8236e-03, -2.4642e-02,  1.5132e-02, -2.3688e-02,\n",
       "          -6.1167e-03, -1.1076e-02,  8.7061e-03, -2.3431e-02, -4.4478e-03,\n",
       "          -0.0000e+00, -1.0170e-02, -1.0563e-02, -4.8033e-03,  2.3436e-02,\n",
       "           9.6102e-03,  8.3305e-03, -1.5551e-02, -1.2596e-02, -9.6159e-03,\n",
       "          -1.7012e-02,  0.0000e+00,  1.8752e-02, -0.0000e+00,  2.1570e-02,\n",
       "           9.5162e-03, -1.6550e-02, -6.8168e-03, -3.3563e-02,  1.2668e-02,\n",
       "          -3.8334e-02, -0.0000e+00,  2.4195e-02,  6.8577e-03,  1.7062e-02,\n",
       "          -2.2017e-02,  0.0000e+00,  1.6191e-03,  1.1218e-02, -6.5747e-03,\n",
       "           2.2128e-03,  8.0248e-03, -1.9437e-02,  9.8646e-03, -2.2639e-02,\n",
       "           1.2925e-02, -2.0261e-02,  2.7327e-02, -0.0000e+00,  5.2029e-02,\n",
       "          -0.0000e+00, -0.0000e+00,  4.3911e-03,  2.0045e-02,  1.6318e-02,\n",
       "          -9.4654e-03,  1.8417e-02,  4.3277e-03, -3.7150e-02,  3.8188e-03,\n",
       "          -2.1430e-03,  6.9191e-04,  1.0033e-02, -1.5628e-02, -2.0913e-02,\n",
       "          -4.1786e-03,  1.2032e-02, -4.9782e-03, -1.5865e-02,  5.7698e-03,\n",
       "          -3.1858e-03,  5.8266e-03,  0.0000e+00, -6.1592e-03,  0.0000e+00,\n",
       "           1.1337e-02, -2.0638e-02,  3.1249e-02, -2.4448e-02, -1.2114e-02,\n",
       "           7.8015e-03, -1.8435e-02, -4.3874e-02, -1.7418e-02,  2.5606e-02,\n",
       "           3.1816e-03,  3.5985e-03, -1.5027e-02, -1.0916e-02, -1.1448e-02,\n",
       "          -3.4760e-03, -4.2417e-03, -2.7838e-02, -2.0666e-02, -1.7128e-02,\n",
       "           2.9093e-02, -2.9627e-02,  1.6813e-02,  1.4260e-02, -5.0944e-02,\n",
       "          -2.2857e-02,  1.7752e-02,  2.6728e-02,  1.6910e-02, -7.1013e-03,\n",
       "           1.4814e-02, -2.5578e-02, -5.5951e-04,  1.7188e-02,  1.0111e-02,\n",
       "           2.7312e-02, -4.1305e-03,  1.5182e-04,  5.9661e-03,  1.8003e-02,\n",
       "           2.6861e-02, -0.0000e+00,  3.0716e-02, -1.9997e-03, -6.3079e-03,\n",
       "          -3.0009e-02, -1.5403e-02,  3.8878e-03,  2.7055e-02,  5.2435e-03,\n",
       "           2.3773e-03,  1.8685e-03,  1.1829e-02,  4.2726e-03, -8.8910e-03,\n",
       "           8.1004e-03,  3.5934e-02,  2.2670e-02,  1.2370e-02, -1.3312e-02,\n",
       "          -0.0000e+00, -7.2670e-03, -3.4268e-03, -3.0136e-03, -0.0000e+00,\n",
       "          -1.4409e-02, -8.0348e-03,  1.0245e-02, -1.6473e-03, -0.0000e+00,\n",
       "           1.1505e-02, -1.8859e-02, -3.0901e-03]]], device='cuda:0',\n",
       "       grad_fn=<NativeDropoutBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_custom_backend(gm, example_inputs):\n",
    "    print(\"My Custom Backend:\")\n",
    "    gm.graph.print_tabular()\n",
    "    return gm.forward\n",
    "\n",
    "mlp = GPT2MLP(64, model._model.config)\n",
    "mlp.to(\"cuda:0\")\n",
    "# mlp.eval()\n",
    "mod = torch.compile(mlp, backend=my_custom_backend)\n",
    "mod(sample_input[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
