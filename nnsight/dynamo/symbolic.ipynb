{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from nnsight.envoy import Envoy\n",
    "from nnsight.util import WrapperModule\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from transformers.utils import fx as tfx\n",
    "import torch.fx as fx\n",
    "from torch.fx import replace_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"EleutherAI/pythia-70m\", device_map=\"cuda:0\", dispatch=True)\n",
    "\n",
    "\n",
    "with model.trace(\"a\"):\n",
    "    # sample_input = model.transformer.h[3].mlp.input.save()\n",
    "    sample_input = model.gpt_neox.layers[3].mlp.input.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = model._model.gpt_neox.layers[3].attention\n",
    "attn_envoy = model._envoy.gpt_neox.layers[3].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hidden_states', 'attention_mask', 'position_ids', 'head_mask', 'layer_past', 'use_cache', 'output_attentions', 'padding_mask']\n",
      "hidden_states\n",
      "attention_mask\n",
      "position_ids\n",
      "head_mask\n",
      "layer_past\n",
      "use_cache\n",
      "output_attentions\n",
      "padding_mask\n"
     ]
    },
    {
     "ename": "TraceError",
     "evalue": "Proxy object cannot be iterated. This can be attempted when the Proxy is used in a loop or as a *args or **kwargs function argument. See the torch.fx docs on pytorch.org for a more detailed explanation of what types of control flow can be traced, and check out the Proxy docstring for help troubleshooting Proxy iteration errors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m symbolic_traced \u001b[38;5;241m=\u001b[39m \u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py:1157\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1157\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1159\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py:824\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    818\u001b[0m             _autowrap_check(\n\u001b[1;32m    819\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    820\u001b[0m             )\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    822\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    823\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 824\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    825\u001b[0m             {},\n\u001b[1;32m    826\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    827\u001b[0m         )\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:182\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions, padding_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# [batch, seq_len, (num_heads * 3 * head_size)]\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m#   --> [batch, seq_len, num_heads, 3 * head_size]\u001b[39;00m\n\u001b[1;32m    181\u001b[0m new_qkv_shape \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size)\n\u001b[0;32m--> 182\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[43mqkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_qkv_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\u001b[39;00m\n\u001b[1;32m    185\u001b[0m query \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/fx/proxy.py:410\u001b[0m, in \u001b[0;36mProxy.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inst\u001b[38;5;241m.\u001b[39margval))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/fx/proxy.py:310\u001b[0m, in \u001b[0;36mTracerBase.iter\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miter\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m    305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being iterated over, such as\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return an iterator.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy object cannot be iterated. This can be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    311\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattempted when the Proxy is used in a loop or\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    312\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as a *args or **kwargs function argument. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    313\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the torch.fx docs on pytorch.org for a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    314\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmore detailed explanation of what types of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    315\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol flow can be traced, and check out the\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    316\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Proxy docstring for help troubleshooting \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    317\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy iteration errors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: Proxy object cannot be iterated. This can be attempted when the Proxy is used in a loop or as a *args or **kwargs function argument. See the torch.fx docs on pytorch.org for a more detailed explanation of what types of control flow can be traced, and check out the Proxy docstring for help troubleshooting Proxy iteration errors"
     ]
    }
   ],
   "source": [
    "symbolic_traced = fx.symbolic_trace(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.is_cross_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = model._model.transformer.h[3].mlp\n",
    "mlp_envoy = model._envoy.transformer.h[3].mlp\n",
    "\n",
    "symbolic_traced : fx.GraphModule = fx.symbolic_trace(mlp)\n",
    "print(symbolic_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "from torch._C import ScriptObject \n",
    "import inspect\n",
    "import torch.utils._pytree as pytree\n",
    "\n",
    "HAS_VARSTUFF = inspect.CO_VARARGS | inspect.CO_VARKEYWORDS\n",
    "\n",
    "_proxyable_classes: Dict[Type, None] = {}\n",
    "\n",
    "test_proxy = 0\n",
    "\n",
    "correct_args = list(attn_envoy._fake_inputs[0][1].keys()) + [\"hidden_states\"]\n",
    "\n",
    "class MyCustomTracer(torch.fx.Tracer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def create_args_for_root(self, root_fn, is_module, concrete_args=None):\n",
    "        print(\"Called create_args_for_root\")\n",
    "        \"\"\"\n",
    "        Create ``placeholder`` nodes corresponding to the signature of the ``root``\n",
    "        Module. This method introspects root's signature and emits those\n",
    "        nodes accordingly, also supporting ``*args`` and ``**kwargs``.\n",
    "        \"\"\"\n",
    "        # In some cases, a function or method has been decorated with a wrapper\n",
    "        # defined via ``functools.wraps``. In this case, the outer code object\n",
    "        # will likely not contain the actual parameters we care about, so unwrap\n",
    "        # the function to get to the innermost callable.\n",
    "        fn_for_analysis = inspect.unwrap(root_fn)\n",
    "        co = fn_for_analysis.__code__\n",
    "        total_args = co.co_argcount + co.co_kwonlyargcount\n",
    "        orig_args = list(co.co_varnames)\n",
    "        names_iter = iter(co.co_varnames)\n",
    "        args: List[Any] = []\n",
    "        skip_arg_idx = 0\n",
    "        if is_module:\n",
    "            if total_args == 0:\n",
    "                raise RuntimeError(\n",
    "                    \"``self`` argument cannot be part of *args expansion!\"\n",
    "                )\n",
    "            skip_arg_idx = 1\n",
    "            next(names_iter)  # skip self\n",
    "            args.append(self.root)\n",
    "\n",
    "        sig = inspect.signature(fn_for_analysis)\n",
    "\n",
    "        def proxy_placeholder(name: str):\n",
    "            if concrete_args is not None and name in concrete_args:\n",
    "                cnt = 0\n",
    "\n",
    "                def replace_ph(x):\n",
    "                    nonlocal cnt\n",
    "                    cnt += 1\n",
    "                    param = sig.parameters[name]\n",
    "                    default = (\n",
    "                        ()\n",
    "                        if param.default is inspect.Parameter.empty\n",
    "                        else (param.default,)\n",
    "                    )\n",
    "                    out = self.create_proxy(\n",
    "                        \"placeholder\", f\"{name}_{str(cnt)}\", default, {}\n",
    "                    )\n",
    "                    if isinstance(x, PHBase):\n",
    "                        def transfer_attrs(fr, to):\n",
    "                            for attr_name in dir(fr):\n",
    "                                attr_val = getattr(fr, attr_name)\n",
    "                                if (\n",
    "                                    not callable(attr_val)\n",
    "                                    and not attr_name.startswith(\"__\")\n",
    "                                    and not hasattr(to, attr_name)\n",
    "                                ):\n",
    "                                    setattr(to, attr_name, attr_val)\n",
    "\n",
    "                        if x != PH:\n",
    "                            # Transfer attrs in the case where you're using a placeholder other\n",
    "                            # than the singleton PH (PH has no attributes to transfer).\n",
    "                            # Proxies were created out of the placeholders.\n",
    "                            # Transfer any metadata (put on the placeholders in the form of\n",
    "                            # attributes set by the user) from the placeholder to the\n",
    "                            # underlying nodes (the proxy is unwrapped by the user, but\n",
    "                            # the metadata should hold).\n",
    "                            transfer_attrs(fr=x, to=out.node)\n",
    "\n",
    "                        return out\n",
    "                    # Union[int, bool] == bool in Python <= 3.6\n",
    "                    if (\n",
    "                        type(x) == bool\n",
    "                        or type(x) in base_types\n",
    "                        and type(x) != torch.Tensor\n",
    "                    ):\n",
    "                        torch._assert(\n",
    "                            out == x,\n",
    "                            f\"{name} has been specialized to have value {x} but got another value\",\n",
    "                        )\n",
    "                    elif type(x) == type(None):\n",
    "                        args = (\n",
    "                            out,\n",
    "                            f\"{name} has been specialized to have value None but got another value\",\n",
    "                        )\n",
    "                        self.create_proxy(\"call_function\", _assert_is_none, args, {})\n",
    "                    else:\n",
    "                        warnings.warn(\n",
    "                            f\"Was not able to add assertion to guarantee correct input {name} to \"\n",
    "                            f\"specialized function. It is up to the user to make sure that your inputs match the \"\n",
    "                            f\"inputs you specialized the function with.\"\n",
    "                        )\n",
    "\n",
    "                    return x\n",
    "\n",
    "                return pytree.tree_map(replace_ph, concrete_args[name])\n",
    "            if name[0] == \"*\":\n",
    "                default = ()\n",
    "            else:\n",
    "                param = sig.parameters[name]\n",
    "                default = () if param.default is inspect.Parameter.empty else (param.default,)  # type: ignore[assignment]\n",
    "            return self.create_proxy(\n",
    "                \"placeholder\",\n",
    "                name,\n",
    "                default,\n",
    "                {},\n",
    "                type_expr=fn_for_analysis.__annotations__.get(name, None)\n",
    "            )\n",
    "\n",
    "        arg_names = [next(names_iter) for idx in range(skip_arg_idx, total_args)]\n",
    "        if isinstance(concrete_args, tuple):\n",
    "            if len(arg_names) != len(concrete_args):\n",
    "                raise RuntimeError(\n",
    "                    f\"Tracing expected {len(arg_names)} arguments but got {len(concrete_args)} concrete arguments\"\n",
    "                )\n",
    "            concrete_args = dict(zip(arg_names, concrete_args))\n",
    "        args.extend(proxy_placeholder(names) for names in arg_names)\n",
    "\n",
    "        if co.co_kwonlyargcount > 0 or co.co_flags & HAS_VARSTUFF:\n",
    "            # TODO: type annotations for *args and **kwargs\n",
    "            if co.co_flags & inspect.CO_VARARGS:\n",
    "                args.append(proxy_placeholder(\"*\" + next(names_iter)))\n",
    "            if co.co_flags & inspect.CO_VARKEYWORDS:\n",
    "                args.append(proxy_placeholder(\"**\" + next(names_iter)))\n",
    "            root_fn = _patch_function(root_fn, len(args))\n",
    "\n",
    "        flat_args, in_spec = pytree.tree_flatten(tuple(args))\n",
    "        if any(not isinstance(i, pytree.LeafSpec) for i in in_spec.children_specs):\n",
    "            # In the case that we have pytree-flattened inputs in\n",
    "            # `concrete_args`, generate a flattening wrapper around the\n",
    "            # original root function and return that.\n",
    "            self.graph._codegen = _PyTreeCodeGen(\n",
    "                _PyTreeInfo(orig_args[:total_args], in_spec, None)\n",
    "            )\n",
    "\n",
    "            def flatten_fn(*args):\n",
    "                tree_args = pytree.tree_unflatten(list(args), in_spec)\n",
    "                tree_out = root_fn(*tree_args)\n",
    "                out_args, out_spec = pytree.tree_flatten(tree_out)\n",
    "                assert isinstance(self.graph._codegen, _PyTreeCodeGen)\n",
    "                self.graph._codegen.pytree_info = (\n",
    "                    self.graph._codegen.pytree_info._replace(out_spec=out_spec)\n",
    "                )\n",
    "                return out_args\n",
    "\n",
    "            return flatten_fn, flat_args\n",
    "\n",
    "        new_args = [args[0]]\n",
    "\n",
    "        test_proxy = args[1]\n",
    "        \n",
    "        for arg in args[1:]:\n",
    "            node = arg.node\n",
    "            name = node.name\n",
    "            if name in correct_args:\n",
    "                new_args.append(arg)\n",
    "                print(name)\n",
    "            else:\n",
    "                new_args.append(None)\n",
    "\n",
    "        \n",
    "        print(new_args)\n",
    "\n",
    "        return root_fn, new_args\n",
    "    \n",
    "    def create_arg(self, a: Any) -> \"Argument\":\n",
    "        \"\"\"\n",
    "        A method to specify the behavior of tracing when preparing values to\n",
    "        be used as arguments to nodes in the ``Graph``.\n",
    "\n",
    "        By default, the behavior includes:\n",
    "\n",
    "        #. Iterate through collection types (e.g. tuple, list, dict) and recursively\n",
    "           call ``create_args`` on the elements.\n",
    "        #. Given a Proxy object, return a reference to the underlying IR ``Node``\n",
    "        #. Given a non-Proxy Tensor object, emit IR for various cases:\n",
    "\n",
    "            * For a Parameter, emit a ``get_attr`` node referring to that Parameter\n",
    "            * For a non-Parameter Tensor, store the Tensor away in a special\n",
    "              attribute referring to that attribute.\n",
    "\n",
    "        This method can be overridden to support more types.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            a (Any): The value to be emitted as an ``Argument`` in the ``Graph``.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            The value ``a`` converted into the appropriate ``Argument``\n",
    "        \"\"\"\n",
    "        # The base tracer is used to construct Graphs when there is no associated\n",
    "        # module hierarchy, so it can never create parameter references.\n",
    "        # The default tracer adds the ability to refer to parameters when\n",
    "        # tracing modules.\n",
    "        if isinstance(a, torch.nn.Parameter):\n",
    "            print(\"Conditional type: torch.nn.Parameter\")\n",
    "            for n, p in self.root.named_parameters():\n",
    "                if a is p:\n",
    "                    return self.create_node(\"get_attr\", n, (), {})\n",
    "            raise NameError(\"parameter is not a member of this module\")\n",
    "\n",
    "        elif isinstance(a, torch.Tensor):\n",
    "            print(\"Conditional type: torch.Tensor\")\n",
    "            for n_, p_ in self.root.named_buffers():\n",
    "                if a is p_:\n",
    "                    return self.create_node(\"get_attr\", n_, (), {})\n",
    "        elif isinstance(a, torch.nn.Module):\n",
    "            print(\"Conditional type: torch.nn.Module\")\n",
    "            for n_, p_ in self.root.named_modules():\n",
    "                if a is p_:\n",
    "                    return self.create_node(\"get_attr\", n_, (), {})\n",
    "        # For NamedTuple instances that appear literally as args, we emit\n",
    "        # a node to construct the NamedTuple and use that Node as the argument.\n",
    "        if isinstance(a, tuple) and hasattr(a, \"_fields\"):\n",
    "            print(\"Conditional type: NamedTuple\")\n",
    "            args = tuple(self.create_arg(elem) for elem in a)\n",
    "            return self.create_node(\"call_function\", a.__class__, args, {})\n",
    "\n",
    "        \n",
    "        # Tensors do not have a reliable string repr() from which they can be\n",
    "        # constructed (and we probably don't want to rely on that, either), so\n",
    "        # for any constant Tensor values we encounter, first search for if they\n",
    "        # are an attribute of some module in the module hierarchy. If so, emit\n",
    "        # a get_attr to retrieve that tensor. Otherwise, we'll store away the\n",
    "        # tensor value into a special attribute on the Module s.t. we can\n",
    "        # retrieve it with a get_attr.\n",
    "        if isinstance(a, (torch.Tensor, ScriptObject)):\n",
    "            qualname: Optional[str] = self.tensor_attrs.get(a)\n",
    "\n",
    "            # Tensor was not found in the Module hierarchy, stow it away in a\n",
    "            # special attribute and set the qualname to refer to that\n",
    "            if not qualname:\n",
    "                i = 0\n",
    "                while True:\n",
    "                    qualname = f\"_tensor_constant{i}\"\n",
    "                    if not hasattr(self.root, qualname):\n",
    "                        break\n",
    "                    i += 1\n",
    "                self.tensor_attrs[a] = qualname\n",
    "                setattr(self.root, qualname, a)\n",
    "\n",
    "            return self.create_node(\"get_attr\", qualname, (), {})\n",
    "\n",
    "        # if type(a) in _proxyable_classes:\n",
    "        #     # This is an instance of a proxyable class for which we did not\n",
    "        #     # witness its construction. Intern this as a constant attribute\n",
    "\n",
    "        #     # TODO: binary search\n",
    "        #     i = 0\n",
    "        #     while True:\n",
    "        #         qualname = f\"_{a.__class__.__name__}_constant_{i}\"\n",
    "        #         if not hasattr(self.root, qualname):\n",
    "        #             break\n",
    "        #         i += 1\n",
    "        #     setattr(self.root, qualname, a)\n",
    "\n",
    "        #     return self.create_node(\"get_attr\", qualname, (), {})\n",
    "\n",
    "        return super().create_arg(a)\n",
    "\n",
    "\n",
    "traced_graph = MyCustomTracer().trace(attn)\n",
    "# trace() returns a Graph. Let's wrap it up in a\n",
    "# GraphModule to make it runnable\n",
    "# traced = torch.fx.GraphModule(mod, traced_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.full(\n",
    "    [], test_proxy,   \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
