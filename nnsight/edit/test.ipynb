{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sub_envoy(envoy, target):\n",
    "    if envoy._module_path == target:\n",
    "        return envoy\n",
    "    \n",
    "    for sub in envoy._sub_envoys:\n",
    "        result = fetch_sub_envoy(sub, target)  \n",
    "        if result is not None:\n",
    "            return result\n",
    "\n",
    "import einops\n",
    "\n",
    "def create_fn_hook(envoy):\n",
    "    split = lambda x: einops.rearrange(x, \"batch (heads d qkv) -> batch heads d qkv\", qkv=3, d=768)\n",
    "    revert = lambda x: einops.rearrange(x, \"batch heads d qkv -> batch (heads d qkv)\", qkv=3, d=768)\n",
    "\n",
    "    hook = FnEnvoy(\n",
    "        envoy,\n",
    "        split,\n",
    "        revert,\n",
    "    )\n",
    "\n",
    "    return hook\n",
    "\n",
    "for layer_idx in range(12): \n",
    "    envoy_path = f\".transformer.h.{layer_idx}.attn.c_attn\"\n",
    "    envoy = fetch_sub_envoy(model._envoy, envoy_path)\n",
    "    hook = create_fn_hook(envoy)\n",
    "\n",
    "    parent_path = f\".transformer.h.{layer_idx}.attn\"\n",
    "    parent = envoy = fetch_sub_envoy(model._envoy, parent_path)\n",
    "    setattr(parent, \"qkv\", hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "def split_qkv(envoy):\n",
    "\n",
    "    split = lambda x: einops.rearrange(x, \"batch (heads d qkv) -> batch heads d qkv\", qkv=3, d=768)\n",
    "    revert = lambda x: einops.rearrange(x, \"batch heads d qkv -> batch (heads d qkv)\", qkv=3, d=768)\n",
    "\n",
    "    wrapped = envoy._tracer._graph.add(\n",
    "        target=split,\n",
    "        args=[\n",
    "            envoy.output[0]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    reverted = envoy._tracer._graph.add(\n",
    "        target=revert,\n",
    "        args=[\n",
    "            wrapped\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return reverted\n",
    "\n",
    "with model.trace() as tracer:\n",
    "    with tracer.invoke(\"hello\"):\n",
    "        out = model.transformer.h[0].attn.c_attn\n",
    "        q = split_qkv(out)\n",
    "\n",
    "        q = q * 1000\n",
    "        # model.transformer.h[0].attn.c_attn.output *= 0.\n",
    "\n",
    "        edited = model.transformer.h[1].attn.input[0][0].save()\n",
    "    with tracer.invoke(\"hello\"):\n",
    "        clean = model.transformer.h[1].attn.input[0][0].save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct out of range 1.0e-12 = 0.00%\n",
      "pct out of range 1.0e-10 = 0.00%\n",
      "pct out of range 1.0e-06 = 0.00%\n",
      "pct out of range 1.0e-04 = 0.00%\n",
      "pct out of range 1.0e-03 = 0.00%\n",
      "pct out of range 1.0e-02 = 0.00%\n",
      "pct out of range 1.0e+00 = 0.00%\n"
     ]
    }
   ],
   "source": [
    "def test_resolution(ground_truth, sample):\n",
    "    for resolution in [1e-12, 1e-10, 1e-6, 1e-4, 1e-3, 1e-2, 1.0]:\n",
    "        pct = (sample - ground_truth > resolution).float().mean().item()\n",
    "        print(f'pct out of range {resolution:.1e} = {pct:.2%}')\n",
    "\n",
    "test_resolution(clean, edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sub_envoy(envoy, target):\n",
    "    if envoy._module_path == target:\n",
    "        return envoy\n",
    "    \n",
    "    for sub in envoy._sub_envoys:\n",
    "        result = fetch_sub_envoy(sub, target)  \n",
    "        if result is not None:\n",
    "            return result\n",
    "\n",
    "import einops\n",
    "\n",
    "def create_fn_hook(envoy, target):\n",
    "    rearrange = lambda x : einops.rearrange(x, \"batch seq (heads head_dim) -> batch heads seq head_dim\", heads=12)\n",
    "    revert = lambda x : (einops.rearrange(x, \"batch heads seq head_dim -> batch seq (heads head_dim)\"),)\n",
    "\n",
    "    hook = FnEnvoy(envoy, target, rearrange, revert)\n",
    "\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(12): \n",
    "    \n",
    "    envoy_path = f\".transformer.h.{layer_idx}.attn.c_proj\"\n",
    "    target_path = f\".transformer.h.{layer_idx + 1}.attn.c_proj\"\n",
    "\n",
    "    envoy = fetch_sub_envoy(model, envoy_path)\n",
    "    target = fetch_sub_envoy(model, target_path)\n",
    "\n",
    "    hook = create_fn_hook(envoy, target)\n",
    "\n",
    "    setattr(envoy, \"heads\", hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (embed): Embedding(50257, 768)\n",
       "    (pos_embed): Embedding(1024, 768)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D(\n",
       "            (heads): Placeholder\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (unembed): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"nn\") as tracer:\n",
    "    \n",
    "    test = model.transformer.layers[0].attn.c_proj.heads.output().save()\n",
    "\n",
    "print(test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
