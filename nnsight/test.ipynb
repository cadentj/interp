{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from nnsight.models.UnifiedTransformer import UnifiedTransformer\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = UnifiedTransformer(\n",
    "    'gpt2-small',\n",
    "    processing=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioi_dataset import IOIDataset\n",
    "\n",
    "N = 25\n",
    "clean_dataset = IOIDataset(\n",
    "    prompt_type='mixed',\n",
    "    N=N,\n",
    "    tokenizer=model.tokenizer,\n",
    "    prepend_bos=False,\n",
    "    seed=1,\n",
    "    device=device\n",
    ")\n",
    "corr_dataset = clean_dataset.gen_flipped_prompts('ABC->XYZ, BAB->XYZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean direction: 2.805180311203003, Corrupt direction: 1.2550485134124756\n",
      "Clean metric: 1.0, Corrupt metric: 0.0\n"
     ]
    }
   ],
   "source": [
    "def ave_logit_diff(\n",
    "    logits: Float[Tensor, 'batch seq d_vocab'],\n",
    "    ioi_dataset: IOIDataset,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    '''\n",
    "        Return average logit difference between correct and incorrect answers\n",
    "    '''\n",
    "    # Get logits for indirect objects\n",
    "    batch_size = logits.size(0)\n",
    "    io_logits = logits[range(batch_size), ioi_dataset.word_idx['end'][:batch_size], ioi_dataset.io_tokenIDs[:batch_size]]\n",
    "    s_logits = logits[range(batch_size), ioi_dataset.word_idx['end'][:batch_size], ioi_dataset.s_tokenIDs[:batch_size]]\n",
    "    # Get logits for subject\n",
    "    logit_diff = io_logits - s_logits\n",
    "    return logit_diff if per_prompt else logit_diff.mean()\n",
    "\n",
    "\n",
    "with t.no_grad():\n",
    "    with model.trace(clean_dataset.toks):\n",
    "        clean_logits = model.output.save()\n",
    "\n",
    "    with model.trace(corr_dataset.toks):\n",
    "        corrupt_logits = model.output.save()\n",
    "\n",
    "clean_logits = clean_logits.value\n",
    "corrupt_logits = corrupt_logits.value\n",
    "\n",
    "clean_logit_diff = ave_logit_diff(clean_logits, clean_dataset).item()\n",
    "corrupt_logit_diff = ave_logit_diff(corrupt_logits, corr_dataset).item()\n",
    "\n",
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq_len d_vocab\"],\n",
    "    corrupted_logit_diff: float = corrupt_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    "    ioi_dataset: IOIDataset = clean_dataset\n",
    " ):\n",
    "    patched_logit_diff = ave_logit_diff(logits, ioi_dataset)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "def negative_ioi_metric(logits: Float[Tensor, \"batch seq_len d_vocab\"]):\n",
    "    return -ioi_metric(logits)\n",
    "    \n",
    "# Get clean and corrupt logit differences\n",
    "with t.no_grad():\n",
    "    clean_metric = ioi_metric(clean_logits, corrupt_logit_diff, clean_logit_diff, clean_dataset)\n",
    "    corrupt_metric = ioi_metric(corrupt_logits, corrupt_logit_diff, clean_logit_diff, corr_dataset)\n",
    "\n",
    "print(f'Clean direction: {clean_logit_diff}, Corrupt direction: {corrupt_logit_diff}')\n",
    "print(f'Clean metric: {clean_metric}, Corrupt metric: {corrupt_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eap\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(eap)\n",
    "\n",
    "graph = eap.EAP(model.cfg, components=[\"head\", \"mlp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-4.2697e-01,  1.1448e+00,  1.3131e-01,  ..., -2.4350e+00,\n",
      "           6.0842e-02, -1.7969e+00],\n",
      "         [-1.1787e-01,  1.4245e+00, -1.0625e+00,  ..., -7.6892e-01,\n",
      "           1.8339e-01, -1.5101e+00],\n",
      "         [ 9.1228e-01,  1.3094e-01, -8.9158e-01,  ..., -4.3268e-02,\n",
      "           6.3102e-01, -1.7191e-01],\n",
      "         ...,\n",
      "         [-3.7074e-01,  6.8026e-02,  1.1891e+00,  ...,  3.2080e+00,\n",
      "           6.8503e-01,  8.4383e-01],\n",
      "         [-4.7524e-01, -7.3930e-02,  1.1964e+00,  ...,  2.9511e+00,\n",
      "           4.7496e-01,  1.0620e+00],\n",
      "         [-5.4846e-01, -1.6747e-01,  1.1752e+00,  ...,  2.7625e+00,\n",
      "           3.4602e-01,  1.1410e+00]],\n",
      "\n",
      "        [[-4.2697e-01,  1.1448e+00,  1.3131e-01,  ..., -2.4350e+00,\n",
      "           6.0842e-02, -1.7969e+00],\n",
      "         [-5.0372e-01, -6.8822e-01, -3.5946e-01,  ...,  3.0826e-01,\n",
      "          -4.7470e-01, -1.8963e+00],\n",
      "         [ 1.1867e+00,  2.8087e-01, -9.6663e-01,  ..., -1.8876e-01,\n",
      "           7.2888e-01, -1.9992e-01],\n",
      "         ...,\n",
      "         [-4.2782e-01,  2.4347e-01,  1.0484e+00,  ...,  3.2014e+00,\n",
      "           7.7604e-01,  8.1659e-01],\n",
      "         [-5.3693e-01,  8.8688e-02,  1.0761e+00,  ...,  2.9273e+00,\n",
      "           6.0415e-01,  1.0204e+00],\n",
      "         [-6.0017e-01, -2.3830e-02,  1.0725e+00,  ...,  2.7276e+00,\n",
      "           4.9008e-01,  1.0867e+00]],\n",
      "\n",
      "        [[-4.2697e-01,  1.1448e+00,  1.3131e-01,  ..., -2.4350e+00,\n",
      "           6.0842e-02, -1.7969e+00],\n",
      "         [-1.0099e+00, -3.7584e-01, -1.3737e+00,  ..., -6.9773e-01,\n",
      "          -3.2750e-01, -1.2967e+00],\n",
      "         [ 9.9194e-01,  2.0535e-01, -1.0151e+00,  ..., -1.6008e-01,\n",
      "           6.0891e-01, -1.2934e-01],\n",
      "         ...,\n",
      "         [-3.6057e-01,  2.8102e-01,  1.0729e+00,  ...,  3.0346e+00,\n",
      "           5.9203e-01,  8.1282e-01],\n",
      "         [-4.5031e-01,  9.0773e-02,  1.0842e+00,  ...,  2.8091e+00,\n",
      "           4.3132e-01,  1.0621e+00],\n",
      "         [-5.1020e-01, -2.8616e-02,  1.0668e+00,  ...,  2.6457e+00,\n",
      "           3.2570e-01,  1.1549e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4591e+00,  1.4295e+00,  3.9731e-01,  ..., -1.2715e+00,\n",
      "           7.6592e-01, -4.5999e-01],\n",
      "         [-1.0342e-01, -4.4947e-01, -8.8178e-01,  ...,  4.7374e-01,\n",
      "          -1.1110e-01,  3.5855e-01],\n",
      "         [-4.0744e-01,  7.2930e-01, -2.4017e-01,  ..., -9.2185e-01,\n",
      "          -3.8303e-01,  1.8301e+00],\n",
      "         ...,\n",
      "         [-4.5967e-01, -7.0682e-03,  9.8938e-01,  ...,  2.6786e+00,\n",
      "           5.1365e-01,  1.3216e+00],\n",
      "         [-4.9645e-01, -7.8868e-02,  1.0162e+00,  ...,  2.5241e+00,\n",
      "           4.2996e-01,  1.3284e+00],\n",
      "         [-5.1503e-01, -1.2483e-01,  1.0215e+00,  ...,  2.4281e+00,\n",
      "           3.6450e-01,  1.3137e+00]],\n",
      "\n",
      "        [[ 1.4591e+00,  1.4295e+00,  3.9731e-01,  ..., -1.2715e+00,\n",
      "           7.6592e-01, -4.5999e-01],\n",
      "         [-1.0342e-01, -4.4947e-01, -8.8178e-01,  ...,  4.7374e-01,\n",
      "          -1.1110e-01,  3.5855e-01],\n",
      "         [-3.9625e-01, -1.1017e+00, -5.0929e-01,  ...,  1.4744e-01,\n",
      "          -5.8459e-01, -1.2930e+00],\n",
      "         ...,\n",
      "         [-6.2678e-02,  1.0424e+00, -2.6256e-01,  ..., -1.4165e+00,\n",
      "           6.9959e-01,  4.4693e-01],\n",
      "         [-1.3646e-01,  1.8663e-01,  1.2138e+00,  ...,  3.0162e+00,\n",
      "           7.6112e-01,  9.2837e-01],\n",
      "         [-3.1148e-01,  3.1768e-02,  1.2087e+00,  ...,  2.7879e+00,\n",
      "           5.9604e-01,  1.1231e+00]],\n",
      "\n",
      "        [[ 1.4591e+00,  1.4295e+00,  3.9731e-01,  ..., -1.2715e+00,\n",
      "           7.6592e-01, -4.5999e-01],\n",
      "         [-1.0342e-01, -4.4947e-01, -8.8178e-01,  ...,  4.7374e-01,\n",
      "          -1.1110e-01,  3.5855e-01],\n",
      "         [ 2.2954e-01,  5.6525e-01, -8.3648e-01,  ..., -9.1104e-01,\n",
      "           2.7813e-01,  2.1732e+00],\n",
      "         ...,\n",
      "         [ 2.7368e-01, -6.7361e-01, -2.9495e-01,  ...,  1.0043e+00,\n",
      "          -1.3524e+00,  1.4618e+00],\n",
      "         [-4.0290e-01,  1.7887e-01,  1.1972e+00,  ...,  2.9733e+00,\n",
      "           7.0432e-01,  8.2404e-01],\n",
      "         [-4.8699e-01, -1.5199e-03,  1.1859e+00,  ...,  2.7688e+00,\n",
      "           5.2670e-01,  1.0515e+00]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "graph.run(\n",
    "    model,\n",
    "    clean_dataset.toks,\n",
    "    corr_dataset.toks,\n",
    "    batch_size=25,\n",
    "    metric=ioi_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.9.9 -> [-0.021] -> head.11.10.q\n",
      "head.10.7 -> [0.02] -> head.11.10.q\n",
      "head.5.5 -> [0.016] -> head.8.6.v\n",
      "mlp.0 -> [0.014] -> head.6.9.q\n",
      "head.9.9 -> [-0.013] -> head.10.7.q\n",
      "head.5.5 -> [-0.011] -> head.6.9.q\n",
      "mlp.0 -> [-0.01] -> head.11.10.k\n",
      "head.9.6 -> [-0.01] -> head.11.10.q\n",
      "head.9.6 -> [-0.009] -> head.10.7.q\n",
      "head.4.11 -> [0.009] -> head.6.9.k\n",
      "mlp.0 -> [-0.007] -> head.10.7.k\n",
      "head.5.5 -> [0.007] -> head.7.9.v\n",
      "mlp.0 -> [0.006] -> head.3.0.k\n",
      "mlp.0 -> [-0.006] -> head.10.7.v\n",
      "head.10.10 -> [-0.006] -> head.11.10.q\n",
      "head.5.5 -> [0.006] -> head.8.10.v\n",
      "mlp.0 -> [-0.006] -> head.7.9.k\n",
      "head.8.6 -> [0.006] -> head.9.9.q\n",
      "mlp.0 -> [0.005] -> head.3.0.q\n",
      "head.6.9 -> [0.005] -> head.8.6.v\n"
     ]
    }
   ],
   "source": [
    "edges = graph.top_edges(n=20, format=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
