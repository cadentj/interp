{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from nnsight.util import WrapperModule, fetch_and_set, fetch_attr\n",
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda:0\", dispatch=True)\n",
    "\n",
    "with model.trace(\"a\"):\n",
    "    h_0_input = model.transformer.h[0].input.save()\n",
    "    attn_input = model.transformer.h[0].attn.input.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edit:\n",
    "\n",
    "    def __init__(self, \n",
    "        parent: str, \n",
    "        target: str, \n",
    "        key: str, \n",
    "        replacement: torch.nn.Module,\n",
    "    ) -> None:\n",
    "        self.parent = parent\n",
    "        self.target = target\n",
    "        self.key = key\n",
    "        self.replacement = replacement\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.parent}.{self.target} -> {self.key}\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModule(torch.nn.Module):\n",
    "    \"\"\"Simple torch module which passes it's input through. Useful for hooking.\n",
    "    If there is only one argument, returns the first element.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "\n",
    "        value = args * 10000000\n",
    "    \n",
    "        print(\"stuff is going through this node!\")\n",
    "\n",
    "        return value\n",
    "\n",
    "wrapper_edit_one = Edit(\n",
    "    \"transformer.h.0.attn\", \n",
    "    \"query\", \n",
    "    \"query_wrapper\",\n",
    "    WrapperModule()\n",
    ")\n",
    "    \n",
    "edits = [wrapper_edit_one]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backend(\n",
    "    edit_batch: List[Edit],\n",
    "):  \n",
    "    # wrapper_dict = {edit.key: edit.replacement for edit in edit_batch}\n",
    "    target_dict = {edit.target: edit.key for edit in edit_batch}\n",
    "\n",
    "    def edited_backend(gm: torch.fx.GraphModule, _: List[torch.Tensor]):\n",
    "        unseen = set(target_dict.keys())\n",
    "        \n",
    "        # print(gm)\n",
    "        for edit in edit_batch:\n",
    "            gm.add_submodule(edit.key, edit.replacement)\n",
    "\n",
    "        for node in gm.graph.nodes:\n",
    "            if node.op == \"placeholder\": \n",
    "                continue\n",
    "            if node.name in unseen:\n",
    "                \n",
    "                with gm.graph.inserting_before(node):\n",
    "                    new = gm.graph.create_node(node.op, node.target, args=node.args, kwargs=node.kwargs, name=\"original_\" + node.name)\n",
    "                    wrapper_node = gm.graph.call_module(target_dict[node.name], args=(new,))\n",
    "                    node.replace_all_uses_with(wrapper_node)\n",
    "                    gm.graph.erase_node(node)\n",
    "\n",
    "                unseen.remove(node.name)\n",
    "            \n",
    "            if not unseen:\n",
    "                break\n",
    "\n",
    "\n",
    "        gm.recompile()\n",
    "        # print(gm)\n",
    "\n",
    "        gm.graph.print_tabular()\n",
    "\n",
    "        return gm.forward\n",
    "\n",
    "    return edited_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model._model.transformer.h[0].attn\n",
    "\n",
    "for edit in grouped_edits[\"transformer.h.0.attn\"]:\n",
    "    setattr(mod, edit.key, edit.replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name              target                                                    args                                      kwargs\n",
      "-------------  ----------------  --------------------------------------------------------  ----------------------------------------  --------\n",
      "placeholder    l_x_              L_x_                                                      ()                                        {}\n",
      "call_method    size              size                                                      (l_x_,)                                   {}\n",
      "get_attr       l__self___bias    L__self___bias                                            ()                                        {}\n",
      "call_method    size_1            size                                                      (l_x_, -1)                                {}\n",
      "call_method    view              view                                                      (l_x_, -1, size_1)                        {}\n",
      "get_attr       l__self___weight  L__self___weight                                          ()                                        {}\n",
      "call_function  x                 <built-in method addmm of type object at 0x7f2dd7e858a0>  (l__self___bias, view, l__self___weight)  {}\n",
      "call_method    x_1               view                                                      (x, (1, 1, 2304))                         {}\n",
      "output         output            output                                                    ((x_1,),)                                 {}\n",
      "opcode         name            target                       args                             kwargs\n",
      "-------------  --------------  ---------------------------  -------------------------------  ----------\n",
      "placeholder    l_stack0_       L_stack0_                    ()                               {}\n",
      "call_method    split           split                        (l_stack0_, 768)                 {'dim': 2}\n",
      "call_function  original_query  <built-in function getitem>  (split, 0)                       {}\n",
      "call_module    query_wrapper   query_wrapper                (original_query,)                {}\n",
      "call_function  key             <built-in function getitem>  (split, 1)                       {}\n",
      "call_function  value           <built-in function getitem>  (split, 2)                       {}\n",
      "call_method    tensor          view                         (query_wrapper, (1, 1, 12, 64))  {}\n",
      "call_method    query_1         permute                      (tensor, 0, 2, 1, 3)             {}\n",
      "call_method    tensor_1        view                         (key, (1, 1, 12, 64))            {}\n",
      "call_method    key_1           permute                      (tensor_1, 0, 2, 1, 3)           {}\n",
      "call_method    size            size                         (value,)                         {}\n",
      "call_method    tensor_2        view                         (value, (1, 1, 12, 64))          {}\n",
      "call_method    value_1         permute                      (tensor_2, 0, 2, 1, 3)           {}\n",
      "output         output          output                       ((query_1, key_1, value_1),)     {}\n",
      "stuff is going through this node!\n",
      "opcode         name            target                                                     args                                                                                                           kwargs\n",
      "-------------  --------------  ---------------------------------------------------------  -------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------\n",
      "placeholder    s0              s0                                                         ()                                                                                                             {}\n",
      "placeholder    s1              s1                                                         ()                                                                                                             {}\n",
      "placeholder    s2              s2                                                         ()                                                                                                             {}\n",
      "placeholder    l_key_          L_key_                                                     ()                                                                                                             {}\n",
      "placeholder    l_query_        L_query_                                                   ()                                                                                                             {}\n",
      "placeholder    s3              s3                                                         ()                                                                                                             {}\n",
      "placeholder    l_value_        L_value_                                                   ()                                                                                                             {}\n",
      "call_method    transpose       transpose                                                  (l_key_, -1, -2)                                                                                               {}\n",
      "call_function  attn_weights    <built-in method matmul of type object at 0x7f2dd7e858a0>  (l_query_, transpose)                                                                                          {}\n",
      "call_method    size            size                                                       (l_value_, -1)                                                                                                 {}\n",
      "call_function  pow_1           <built-in function pow>                                    (size, 0.5)                                                                                                    {}\n",
      "call_function  full            <built-in method full of type object at 0x7f2dd7e858a0>    ([], pow_1)                                                                                                    {'dtype': torch.float32, 'device': device(type='cuda', index=0)}\n",
      "call_function  attn_weights_1  <built-in function truediv>                                (attn_weights, full)                                                                                           {}\n",
      "get_attr       l__self___bias  L__self___bias                                             ()                                                                                                             {}\n",
      "call_function  causal_mask     <built-in function getitem>                                (l__self___bias, (slice(None, None, None), slice(None, None, None), slice(0, 1, None), slice(None, 1, None)))  {}\n",
      "call_function  mask_value      <built-in method full of type object at 0x7f2dd7e858a0>    ([], -3.4028234663852886e+38)                                                                                  {'dtype': torch.float32, 'device': device(type='cuda', index=0)}\n",
      "call_method    to              to                                                         (attn_weights_1, torch.float32)                                                                                {}\n",
      "call_function  attn_weights_2  <built-in method where of type object at 0x7f2dd7e858a0>   (causal_mask, to, mask_value)                                                                                  {}\n",
      "call_function  attn_weights_3  <function softmax at 0x7f2cfd589580>                       (attn_weights_2,)                                                                                              {'dim': -1}\n",
      "call_method    attn_weights_4  type                                                       (attn_weights_3, torch.float32)                                                                                {}\n",
      "output         output          output                                                     ((attn_weights_4,),)                                                                                           {}\n",
      "opcode         name          target                                                     args                      kwargs\n",
      "-------------  ------------  ---------------------------------------------------------  ------------------------  --------\n",
      "placeholder    s0            s0                                                         ()                        {}\n",
      "placeholder    attn_weights  L_stack0_                                                  ()                        {}\n",
      "placeholder    s1            s1                                                         ()                        {}\n",
      "placeholder    s2            s2                                                         ()                        {}\n",
      "placeholder    l_value_      L_value_                                                   ()                        {}\n",
      "call_function  attn_output   <built-in method matmul of type object at 0x7f2dd7e858a0>  (attn_weights, l_value_)  {}\n",
      "output         output        output                                                     ((attn_output,),)         {}\n",
      "opcode       name           target       args                       kwargs\n",
      "-----------  -------------  -----------  -------------------------  --------\n",
      "placeholder  s0             s0           ()                         {}\n",
      "placeholder  s1             s1           ()                         {}\n",
      "placeholder  attn_output    L_stack0_0_  ()                         {}\n",
      "call_method  permute        permute      (attn_output, 0, 2, 1, 3)  {}\n",
      "call_method  tensor         contiguous   (permute,)                 {}\n",
      "call_method  size           size         (tensor,)                  {}\n",
      "call_method  attn_output_1  view         (tensor, (1, 1, 768))      {}\n",
      "output       output         output       ((attn_output_1,),)        {}\n",
      "opcode         name              target                                                    args                                      kwargs\n",
      "-------------  ----------------  --------------------------------------------------------  ----------------------------------------  --------\n",
      "placeholder    l_x_              L_x_                                                      ()                                        {}\n",
      "call_method    size              size                                                      (l_x_,)                                   {}\n",
      "get_attr       l__self___bias    L__self___bias                                            ()                                        {}\n",
      "call_method    size_1            size                                                      (l_x_, -1)                                {}\n",
      "call_method    view              view                                                      (l_x_, -1, size_1)                        {}\n",
      "get_attr       l__self___weight  L__self___weight                                          ()                                        {}\n",
      "call_function  x                 <built-in method addmm of type object at 0x7f2dd7e858a0>  (l__self___bias, view, l__self___weight)  {}\n",
      "call_method    x_1               view                                                      (x, (1, 1, 768))                          {}\n",
      "output         output            output                                                    ((x_1,),)                                 {}\n"
     ]
    }
   ],
   "source": [
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(model._model.transformer.h[0].attn, backend=get_backend(grouped_edits[\"transformer.h.0.attn\"]), dynamic=True)\n",
    "\n",
    "gm = opt_model(attn_input.value[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (query_wrapper): WrapperModule()\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._model.transformer.h[0].attn = opt_model\n",
    "\n",
    "from nnsight.envoy import Envoy\n",
    "model._envoy = Envoy(model._model)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name            target                       args                             kwargs\n",
      "-------------  --------------  ---------------------------  -------------------------------  ----------\n",
      "placeholder    l_stack0_       L_stack0_                    ()                               {}\n",
      "call_method    split           split                        (l_stack0_, 768)                 {'dim': 2}\n",
      "call_function  original_query  <built-in function getitem>  (split, 0)                       {}\n",
      "call_module    query_wrapper   query_wrapper                (original_query,)                {}\n",
      "call_function  key             <built-in function getitem>  (split, 1)                       {}\n",
      "call_function  value           <built-in function getitem>  (split, 2)                       {}\n",
      "call_method    tensor          view                         (query_wrapper, (1, 1, 12, 64))  {}\n",
      "call_method    query_1         permute                      (tensor, 0, 2, 1, 3)             {}\n",
      "call_method    tensor_1        view                         (key, (1, 1, 12, 64))            {}\n",
      "call_method    key_1           permute                      (tensor_1, 0, 2, 1, 3)           {}\n",
      "call_method    size            size                         (value,)                         {}\n",
      "call_method    tensor_2        view                         (value, (1, 1, 12, 64))          {}\n",
      "call_method    value_1         permute                      (tensor_2, 0, 2, 1, 3)           {}\n",
      "output         output          output                       ((query_1, key_1, value_1),)     {}\n",
      "stuff is going through this node!\n",
      "opcode         name               target                                                     args                                                                                                           kwargs\n",
      "-------------  -----------------  ---------------------------------------------------------  -------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------\n",
      "placeholder    s0                 s0                                                         ()                                                                                                             {}\n",
      "placeholder    s1                 s1                                                         ()                                                                                                             {}\n",
      "placeholder    s2                 s2                                                         ()                                                                                                             {}\n",
      "placeholder    l_key_             L_key_                                                     ()                                                                                                             {}\n",
      "placeholder    l_query_           L_query_                                                   ()                                                                                                             {}\n",
      "placeholder    s3                 s3                                                         ()                                                                                                             {}\n",
      "placeholder    l_value_           L_value_                                                   ()                                                                                                             {}\n",
      "placeholder    l_attention_mask_  L_attention_mask_                                          ()                                                                                                             {}\n",
      "call_method    transpose          transpose                                                  (l_key_, -1, -2)                                                                                               {}\n",
      "call_function  attn_weights       <built-in method matmul of type object at 0x7f2dd7e858a0>  (l_query_, transpose)                                                                                          {}\n",
      "call_method    size               size                                                       (l_value_, -1)                                                                                                 {}\n",
      "call_function  pow_1              <built-in function pow>                                    (size, 0.5)                                                                                                    {}\n",
      "call_function  full               <built-in method full of type object at 0x7f2dd7e858a0>    ([], pow_1)                                                                                                    {'dtype': torch.float32, 'device': device(type='cuda', index=0)}\n",
      "call_function  attn_weights_1     <built-in function truediv>                                (attn_weights, full)                                                                                           {}\n",
      "get_attr       l__self___bias     L__self___bias                                             ()                                                                                                             {}\n",
      "call_function  causal_mask        <built-in function getitem>                                (l__self___bias, (slice(None, None, None), slice(None, None, None), slice(0, 1, None), slice(None, 1, None)))  {}\n",
      "call_function  mask_value         <built-in method full of type object at 0x7f2dd7e858a0>    ([], -3.4028234663852886e+38)                                                                                  {'dtype': torch.float32, 'device': device(type='cuda', index=0)}\n",
      "call_method    to                 to                                                         (attn_weights_1, torch.float32)                                                                                {}\n",
      "call_function  attn_weights_2     <built-in method where of type object at 0x7f2dd7e858a0>   (causal_mask, to, mask_value)                                                                                  {}\n",
      "call_function  attn_weights_3     <built-in function add>                                    (attn_weights_2, l_attention_mask_)                                                                            {}\n",
      "call_function  attn_weights_4     <function softmax at 0x7f2cfd589580>                       (attn_weights_3,)                                                                                              {'dim': -1}\n",
      "call_method    attn_weights_5     type                                                       (attn_weights_4, torch.float32)                                                                                {}\n",
      "output         output             output                                                     ((attn_weights_5,),)                                                                                           {}\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"empty\", scan=False, validate=False):\n",
    "    test = model.transformer.h[0].attn._orig_mod.query_wrapper.output.save()\n",
    "    out_two = model.transformer.h[0].attn.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[0] == out_two[0]).sum() / out[0].numel() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
