{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09db4a7a-4e6a-43f8-b217-2f942d1538a3",
   "metadata": {},
   "source": [
    "# Distributed Alignment Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8374691-18f6-4841-8bb7-a255528967ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import nnsight\n",
    "from nnsight import AbstractModel, LanguageModel, util\n",
    "# from nnsight.Module import Module\n",
    "Module = nnsight.Module\n",
    "from nnsight.toolbox.optim.lora import LORA\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import Dataset as hf_Dataset\n",
    "from das_utils import factual_sampler\n",
    "\n",
    "# If using llama 7b\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07da8235-7d9e-4726-8eb2-fcd92d6274e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33755365376, 34079899648)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4146fc73-b96f-4bf0-ac6c-bc6db6dc8087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3244419c16484daed5358cfc013c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a689a18-c5bc-4bce-9251-e5faa7182053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e8f3a09e0d4993b5595b1f79ec887c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LanguageModel('meta-llama/Llama-2-7b-hf', device_map=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f30c6f4-48ec-4ec2-baa1-5f15bc502cbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_tokens = 10\n",
    "epochs = 1\n",
    "answer = \"Paris\"\n",
    "answer_tokens = model.tokenizer(answer)\n",
    "answer_token = answer_tokens[\"input_ids\"][0]\n",
    "\n",
    "lora = LORA(model.transformer.h[0].mlp, 10)\n",
    "\n",
    "optimizer = torch.optim.AdamW(lora.parameters(), lr=.1)\n",
    "dataset = [[\" \".join([\"_\"] * n_tokens), answer_token]] * 100\n",
    "dataloader = DataLoader(dataset, batch_size=10)\n",
    "\n",
    "\n",
    "lossfn = util.cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7f74df8-b862-4d6a-84a1-efb605717836",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  0\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  1\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  2\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  3\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  4\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  5\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  6\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  7\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  8\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n",
      "  9\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)\n",
      "tensor(nan, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        print(f\"  {i}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with model.forward(inference=False) as runner:\n",
    "            with runner.invoke(inputs) as invoker:\n",
    "\n",
    "                lora()\n",
    "    \n",
    "                logits = model.lm_head.output.save()\n",
    "\n",
    "        print(lora.WA)\n",
    "        loss = lossfn(logits.value[:, -1], targets)\n",
    "        print(loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70650317-f4ce-4786-92ca-c29452e0a415",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _!\n"
     ]
    }
   ],
   "source": [
    "with model.generate() as generator:\n",
    "    with generator.invoke(dataset[0][0]) as invoker:\n",
    "        pass\n",
    "\n",
    "print(model.tokenizer.decode(generator.output[0]))\n",
    "\n",
    "\n",
    "with model.generate() as generator:\n",
    "    with generator.invoke(dataset[0][0]) as invoker:\n",
    "        lora()\n",
    "\n",
    "print(model.tokenizer.decode(generator.output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3ca1d3-43ab-410f-8b05-d43664b57476",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prealign = factual_sampler(model.tokenizer, 5000, game=\"pricing_tag\")\n",
    "prealign_dataset = hf_Dataset.from_dict(\n",
    "    {\"input_ids\": raw_prealign[0], \"labels\": raw_prealign[1]})\n",
    "prealign_dataset.set_format('torch', columns=['input_ids','labels'])\n",
    "prealign_dataloader = DataLoader(\n",
    "    prealign_dataset, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43416d7-4f02-4609-88c8-528cc42545be",
   "metadata": {},
   "outputs": [],
   "source": [
    "prealign_input_batches = torch.split(prealign_dataset[\"input_ids\"], int(5000/1000), dim=0)\n",
    "prealign_labels_batches = torch.split(prealign_dataset['labels'][:,-1], int(5000/1000), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c373259-c166-49e1-b43d-0ca6bf1cfcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease say yes only if it costs between 1.77 and 8.76 dollars, otherwise no.\\n\\n### Input:\\n1.31 dollars\\n\\n### Response:\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(raw_prealign[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "657ab219-4031-4259-97ae-9a7533974719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: Yes | Truth: No\n",
      "Pred: Yes | Truth: No\n",
      "Pred: \n",
      " | Truth: No\n",
      "Pred: Yes | Truth: No\n",
      "Pred: No | Truth: No\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,20):\n",
    "    with model.generate(max_new_tokens=1) as generator:\n",
    "        with generator.invoke(raw_prealign[0][i]) as invoker:\n",
    "            pass\n",
    "    \n",
    "    out = generator.output[:,-1:].cpu()\n",
    "    print(f\"Pred: {model.tokenizer.decode(out[0])} | Truth: {model.tokenizer.decode(raw_prealign[1][i][-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d658a931-3b51-48f0-b643-a25b4ee5b30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "per_batch_acc = []\n",
    "per_batch_res = []\n",
    "total_correct = 0\n",
    "\n",
    "for i, batch in enumerate(prealign_input_batches):\n",
    "    with model.generate(max_new_tokens=1) as generator:\n",
    "        with generator.invoke(batch) as invoker:\n",
    "            pass\n",
    "            \n",
    "    out = generator.output[:,-1].cpu()\n",
    "    correct = (out==prealign_labels_batches[0]).sum()\n",
    "    per_batch_acc.append(correct)\n",
    "    total_correct += (correct)\n",
    "\n",
    "    if i == 10: break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a91e262-3ab1-4384-9cf8-0bd70ad9fbb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'das' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m15\u001b[39m]\u001b[38;5;241m.\u001b[39moutput\n\u001b[0;32m----> 3\u001b[0m rotatedSpaceIntervention \u001b[38;5;241m=\u001b[39m \u001b[43mdas\u001b[49m\u001b[38;5;241m.\u001b[39mBoundlessRotatedSpaceIntervention()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'das' is not defined"
     ]
    }
   ],
   "source": [
    "module = model.model.layers[15]\n",
    "\n",
    "rotatedSpaceIntervention = das.BoundlessRotatedSpaceIntervention(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15601b2-a7cb-4e4d-8988-a635090590fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = int(len(train_dataloader) * 3)\n",
    "warm_up_steps = 0.1 * t_total\n",
    "optimizer_params = []\n",
    "for k, v in alignable.interventions.items():\n",
    "    optimizer_params += [{'params': v[0].rotate_layer.parameters()}]\n",
    "    optimizer_params += [{'params': v[0].intervention_boundaries, 'lr': 1e-2}]\n",
    "optimizer = torch.optim.Adam(\n",
    "    optimizer_params,\n",
    "    lr=1e-3\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warm_up_steps,\n",
    "    num_training_steps=t_total\n",
    ")\n",
    "\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 4\n",
    "total_step = 0\n",
    "target_total_step = len(train_dataloader) * epochs\n",
    "temperature_start = 50.0\n",
    "temperature_end = 0.1\n",
    "temperature_schedule = torch.linspace(\n",
    "    temperature_start, temperature_end, target_total_step\n",
    ").to(torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "# is this correct\n",
    "alignable.set_temperature(temperature_schedule[total_step])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bb752-5aec-4fe1-8b61-da94c5028cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignable.model.train() # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(alignable.model))\n",
    "print(\"intervention trainable parameters: \", alignable.count_parameters())\n",
    "train_iterator = trange(\n",
    "    0, int(epochs), desc=\"Epoch\"\n",
    ")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        _, counterfactual_outputs = alignable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            [{\"input_ids\": inputs[\"source_input_ids\"]}],\n",
    "            {\"sources->base\": ([[[80]]*b_s], [[[80]]*b_s])} # swap 80th token\n",
    "        )\n",
    "        eval_metrics = compute_metrics(\n",
    "            [counterfactual_outputs.logits], [inputs['labels']]\n",
    "        )\n",
    "        \n",
    "        # loss and backprop\n",
    "        loss = calculate_loss(\n",
    "            counterfactual_outputs.logits, inputs[\"labels\"]\n",
    "        )\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({'loss': loss_str, 'acc': eval_metrics[\"accuracy\"]})\n",
    "        \n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                alignable.set_zero_grad()\n",
    "                alignable.set_temperature(temperature_schedule[total_step])\n",
    "        total_step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
