{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92fe721-b4b5-41c4-a675-8fa287291b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from rich import print\n",
    "\n",
    "import time\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff01d7-a029-4d03-9601-cff28b6b7807",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Load in the dataset of factual prompts and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0693180b-b6b3-4d8b-959c-693086f94bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://rome.baulab.info/data/dsets/known_1000.json\")\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ec8a1c-924f-4341-b71a-4a5968b36640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>known_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>attribute</th>\n",
       "      <th>template</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prompt</th>\n",
       "      <th>relation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Vinson Massif</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>{} is located in the continent</td>\n",
       "      <td>of Antarctica. It is the largest of the three</td>\n",
       "      <td>Vinson Massif is located in the continent of</td>\n",
       "      <td>P30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beats Music</td>\n",
       "      <td>Apple</td>\n",
       "      <td>{} is owned by</td>\n",
       "      <td>Apple, which is also the owner of Beats Elect...</td>\n",
       "      <td>Beats Music is owned by</td>\n",
       "      <td>P127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Audible.com</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>{} is owned by</td>\n",
       "      <td>Amazon.com, Inc. or its affiliates.</td>\n",
       "      <td>Audible.com is owned by</td>\n",
       "      <td>P127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Big Bang Theory</td>\n",
       "      <td>CBS</td>\n",
       "      <td>{} premieres on</td>\n",
       "      <td>CBS on September 22.&lt;|endoftext|&gt;</td>\n",
       "      <td>The Big Bang Theory premieres on</td>\n",
       "      <td>P449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MacApp</td>\n",
       "      <td>Apple</td>\n",
       "      <td>{}, a product created by</td>\n",
       "      <td>Apple to help developers create apps for the ...</td>\n",
       "      <td>MacApp, a product created by</td>\n",
       "      <td>P178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   known_id              subject   attribute                        template  \\\n",
       "0         0        Vinson Massif  Antarctica  {} is located in the continent   \n",
       "1         1          Beats Music       Apple                  {} is owned by   \n",
       "2         2          Audible.com      Amazon                  {} is owned by   \n",
       "3         3  The Big Bang Theory         CBS                 {} premieres on   \n",
       "4         4               MacApp       Apple        {}, a product created by   \n",
       "\n",
       "                                          prediction  \\\n",
       "0      of Antarctica. It is the largest of the three   \n",
       "1   Apple, which is also the owner of Beats Elect...   \n",
       "2                Amazon.com, Inc. or its affiliates.   \n",
       "3                  CBS on September 22.<|endoftext|>   \n",
       "4   Apple to help developers create apps for the ...   \n",
       "\n",
       "                                         prompt relation_id  \n",
       "0  Vinson Massif is located in the continent of         P30  \n",
       "1                       Beats Music is owned by        P127  \n",
       "2                       Audible.com is owned by        P127  \n",
       "3              The Big Bang Theory premieres on        P449  \n",
       "4                  MacApp, a product created by        P178  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e71ad0-47a5-485f-b3c5-2ef3b3dd503a",
   "metadata": {},
   "source": [
    "Write a function to sample random prompts and their answers from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c92e3d-0af1-471c-9c8b-24dcebc49701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_prompts(frame: pd.DataFrame, number: Int, prepend_space=False):\n",
    "    intermediate_frame = df.sample(n=number, random_state=10)\n",
    "    intermediate_frame = intermediate_frame[[\"subject\", \"attribute\", \"prompt\", \"template\"]]\n",
    "\n",
    "    prompts = list(intermediate_frame.prompt)\n",
    "    answers = list(intermediate_frame.attribute)\n",
    "\n",
    "    if prepend_space:\n",
    "        answers = [\" \" + a for a in answers]\n",
    "\n",
    "    return prompts, answers, intermediate_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60fd23b8-0dc2-41a1-8e55-8f892ab1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = LanguageModel(\"gpt2\", device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c769325-3c33-4ad0-9e16-6c0dda278cf3",
   "metadata": {},
   "source": [
    "## Running the Dataset\n",
    "\n",
    "Run a single prompt through the model. And observe its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675f0120-2b50-4a84-b794-78f8df013ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "prompts, answers, _ = aggregate_prompts(df, 1, prepend_space=True)\n",
    "\n",
    "with model.forward() as runner:\n",
    "    with runner.invoke(prompts) as invoker: \n",
    "        pass\n",
    "    \n",
    "logits = runner.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04367cfc-a1d9-47e4-a257-12a8619adaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test a prompt on a model and generate a textual representaiton of outputs\n",
    "def test_prompt(prompt: str, answer: str, model):\n",
    "\n",
    "    # Pass prompt through model\n",
    "    with model.forward() as runner:\n",
    "        with runner.invoke(prompt) as invoker: \n",
    "            pass\n",
    "            \n",
    "    # Get logits of output\n",
    "    logits = runner.output[0]\n",
    "    logits = logits[0,-1,:] # only over the final token\n",
    "    probs = logits.softmax(dim=-1)\n",
    "\n",
    "    # Sort probabilities descending\n",
    "    sorted_indices = t.argsort(probs, descending=True)\n",
    "    # Tokenize answer\n",
    "    val = model.tokenizer(answer).input_ids[0]\n",
    "    \n",
    "    # Get probability at answer token and print\n",
    "    rank = (sorted_indices == val).nonzero(as_tuple=True)[0]\n",
    "    print(f\"[bold]Rank: {rank.item()}        Logit: {logits[val].item():.3f} Prob: {probs[val].item():.3f} Token: |{model.tokenizer.decode(val)}|[/]\")\n",
    "\n",
    "    # Print scores for top 10 values\n",
    "    for i, (tok, prob) in enumerate(zip(probs.topk(10).indices, probs.topk(10).values)):\n",
    "        print(f\"Top {i}th token. Logit: {logits[tok]:.3f} Prob: {prob:.3f} Token: |{model.tokenizer.decode(tok)}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace3c236-7b81-46ba-aa1e-7fb28d25c49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -29.5320,  -29.1415,  -32.1064,  ...,  -36.6483,  -36.0686,\n",
       "           -29.4142],\n",
       "         [ -97.0270,  -94.3519,  -98.3774,  ...,  -97.9037, -101.1202,\n",
       "           -97.1184],\n",
       "         [ -93.0036,  -93.6977, -100.1058,  ..., -105.2055,  -97.9785,\n",
       "           -92.5265],\n",
       "         [-114.6407, -115.6995, -117.0248,  ..., -122.8961, -114.2963,\n",
       "          -111.1887]]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with model.forward() as runner:\n",
    "    with runner.invoke(prompts[0]) as invoker:\n",
    "        pass\n",
    "\n",
    "runner.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fa78d0b-c26c-4266-b2e1-2c07a47a5898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ankara'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(runner.output[0][:,-1,:].softmax(-1).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90fb3d7-bea0-44e9-8762-ad345507091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Egypt's capital, Ankara\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with model.generate(max_new_tokens=1) as generator:\n",
    "    with generator.invoke(prompts[0]) as invoker:\n",
    "        pass\n",
    "\n",
    "model.tokenizer.decode(generator.output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c801ac32-5df1-46cb-8024-8067d9c29b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-104.812</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017</span><span style=\"font-weight: bold\"> Token: | Cairo|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRank: \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m-104.812\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m0.017\u001b[0m\u001b[1m Token: | Cairo|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 0th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-103.385</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.071</span> Token: | Ankara|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 0th token. Logit: \u001b[1;36m-103.385\u001b[0m Prob: \u001b[1;36m0.071\u001b[0m Token: | Ankara|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 1th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-103.440</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.068</span> Token: | Moscow|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 1th token. Logit: \u001b[1;36m-103.440\u001b[0m Prob: \u001b[1;36m0.068\u001b[0m Token: | Moscow|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 2th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-103.674</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.054</span> Token: | Damascus|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 2th token. Logit: \u001b[1;36m-103.674\u001b[0m Prob: \u001b[1;36m0.054\u001b[0m Token: | Damascus|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 3th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-103.921</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.042</span> Token: | Kiev|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 3th token. Logit: \u001b[1;36m-103.921\u001b[0m Prob: \u001b[1;36m0.042\u001b[0m Token: | Kiev|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 4th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-104.203</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032</span> Token: | Tehran|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 4th token. Logit: \u001b[1;36m-104.203\u001b[0m Prob: \u001b[1;36m0.032\u001b[0m Token: | Tehran|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 5th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-104.206</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.031</span> Token: | Riyadh|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 5th token. Logit: \u001b[1;36m-104.206\u001b[0m Prob: \u001b[1;36m0.031\u001b[0m Token: | Riyadh|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 6th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-104.303</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.029</span> Token: | Sana|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 6th token. Logit: \u001b[1;36m-104.303\u001b[0m Prob: \u001b[1;36m0.029\u001b[0m Token: | Sana|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 7th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-104.671</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.020</span> Token: | D|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 7th token. Logit: \u001b[1;36m-104.671\u001b[0m Prob: \u001b[1;36m0.020\u001b[0m Token: | D|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 8th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-104.812</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017</span> Token: | Cairo|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 8th token. Logit: \u001b[1;36m-104.812\u001b[0m Prob: \u001b[1;36m0.017\u001b[0m Token: | Cairo|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 9th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-104.841</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.017</span> Token: | and|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 9th token. Logit: \u001b[1;36m-104.841\u001b[0m Prob: \u001b[1;36m0.017\u001b[0m Token: | and|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.tokenizer(answers).input_ids[0]\n",
    "test_prompt(prompts[0], answers[0], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefefe5-63b0-426d-b53c-5ec562f39f7a",
   "metadata": {},
   "source": [
    "### Exercise: Calculate probabilities across a batch of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fa531bc-37c4-4ad1-baf3-dc60ee235c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, answers, _ = aggregate_prompts(df, 200, prepend_space=True)\n",
    "\n",
    "with model.generate(max_new_tokens=1) as generator:\n",
    "    with generator.invoke(prompts) as invoker:\n",
    "        out = model.lm_head.output.t[-1].save()\n",
    "\n",
    "out = out.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0256e37f-c70b-4d4e-9594-b058f0e40b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1042, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = out.softmax(dim=-1)\n",
    "indices_tensor = t.tensor([model.tokenizer.encode(a) for a in answers]).to(device)\n",
    "top_probs = t.gather(probs, 1, indices_tensor)\n",
    "top_probs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4c84a-741e-451c-a97f-aa1daa084edd",
   "metadata": {},
   "source": [
    "Broken runs with forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075b372f-fd2e-4fbf-89c9-318fbbda097d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Saint-Marcellin was created in the country of'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Law &amp; Order was released on'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Knowledge Graph is owned by'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Saint Valentine holds the position of the first'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Francis Blanche speaks during a news conference at the'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Saint-Marcellin was created in the country of'\u001b[0m,\n",
       "    \u001b[32m'Law & Order was released on'\u001b[0m,\n",
       "    \u001b[32m'Knowledge Graph is owned by'\u001b[0m,\n",
       "    \u001b[32m'Saint Valentine holds the position of the first'\u001b[0m,\n",
       "    \u001b[32m'Francis Blanche speaks during a news conference at the'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saint-Marcellin was created in the country of\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saint-Marcellin was created in the country of\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m out_a \u001b[38;5;241m=\u001b[39m out_a\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     10\u001b[0m out_b \u001b[38;5;241m=\u001b[39m invoker\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnew_out\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(out_b[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\u001b[38;5;241m.\u001b[39margmax())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mforward(prompts[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m invoker:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_out' is not defined"
     ]
    }
   ],
   "source": [
    "prompts, answers = aggregate_prompts(df, 100, prepend_space=True)\n",
    "\n",
    "print(prompts[0:5])\n",
    "print(prompts[0])\n",
    "\n",
    "with model.forward(prompts) as invoker:\n",
    "    out_a = model.lm_head.output.t[-1].save()\n",
    "\n",
    "out_a = out_a.value\n",
    "out_b = invoker.output[0]\n",
    "\n",
    "print(new_out[0].argmax())\n",
    "print(out_b[0,-1,:].argmax())\n",
    "\n",
    "with model.forward(prompts[0]) as invoker:\n",
    "    out_c = model.lm_head.output.t[-1].save()\n",
    "\n",
    "out_c = out_c.value\n",
    "out_d = invoker.output[0]\n",
    "\n",
    "print(out_c[0].argmax())\n",
    "print(out_d[0,-1,:].argmax())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ed2fb-fc42-436a-9a09-58496e743729",
   "metadata": {},
   "source": [
    "# Module 2 (Activation Patching)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
