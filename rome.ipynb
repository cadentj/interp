{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92fe721-b4b5-41c4-a675-8fa287291b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch as t\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from rich import print\n",
    "\n",
    "import time\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff01d7-a029-4d03-9601-cff28b6b7807",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Load in the dataset of factual prompts and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0693180b-b6b3-4d8b-959c-693086f94bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://rome.baulab.info/data/dsets/known_1000.json\")\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ec8a1c-924f-4341-b71a-4a5968b36640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>known_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>attribute</th>\n",
       "      <th>template</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prompt</th>\n",
       "      <th>relation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Vinson Massif</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>{} is located in the continent</td>\n",
       "      <td>of Antarctica. It is the largest of the three</td>\n",
       "      <td>Vinson Massif is located in the continent of</td>\n",
       "      <td>P30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Beats Music</td>\n",
       "      <td>Apple</td>\n",
       "      <td>{} is owned by</td>\n",
       "      <td>Apple, which is also the owner of Beats Elect...</td>\n",
       "      <td>Beats Music is owned by</td>\n",
       "      <td>P127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Audible.com</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>{} is owned by</td>\n",
       "      <td>Amazon.com, Inc. or its affiliates.</td>\n",
       "      <td>Audible.com is owned by</td>\n",
       "      <td>P127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Big Bang Theory</td>\n",
       "      <td>CBS</td>\n",
       "      <td>{} premieres on</td>\n",
       "      <td>CBS on September 22.&lt;|endoftext|&gt;</td>\n",
       "      <td>The Big Bang Theory premieres on</td>\n",
       "      <td>P449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MacApp</td>\n",
       "      <td>Apple</td>\n",
       "      <td>{}, a product created by</td>\n",
       "      <td>Apple to help developers create apps for the ...</td>\n",
       "      <td>MacApp, a product created by</td>\n",
       "      <td>P178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   known_id              subject   attribute                        template  \\\n",
       "0         0        Vinson Massif  Antarctica  {} is located in the continent   \n",
       "1         1          Beats Music       Apple                  {} is owned by   \n",
       "2         2          Audible.com      Amazon                  {} is owned by   \n",
       "3         3  The Big Bang Theory         CBS                 {} premieres on   \n",
       "4         4               MacApp       Apple        {}, a product created by   \n",
       "\n",
       "                                          prediction  \\\n",
       "0      of Antarctica. It is the largest of the three   \n",
       "1   Apple, which is also the owner of Beats Elect...   \n",
       "2                Amazon.com, Inc. or its affiliates.   \n",
       "3                  CBS on September 22.<|endoftext|>   \n",
       "4   Apple to help developers create apps for the ...   \n",
       "\n",
       "                                         prompt relation_id  \n",
       "0  Vinson Massif is located in the continent of         P30  \n",
       "1                       Beats Music is owned by        P127  \n",
       "2                       Audible.com is owned by        P127  \n",
       "3              The Big Bang Theory premieres on        P449  \n",
       "4                  MacApp, a product created by        P178  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e71ad0-47a5-485f-b3c5-2ef3b3dd503a",
   "metadata": {},
   "source": [
    "Write a function to sample random prompts and their answers from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c92e3d-0af1-471c-9c8b-24dcebc49701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_prompts(frame: pd.DataFrame, number: Int, prepend_space=False):\n",
    "    intermediate_frame = df.sample(n=number, random_state=2)\n",
    "    intermediate_frame = intermediate_frame[[\"subject\", \"attribute\", \"prompt\"]]\n",
    "\n",
    "    prompts = list(intermediate_frame.prompt)\n",
    "    answers = list(intermediate_frame.attribute)\n",
    "\n",
    "    if prepend_space:\n",
    "        answers = [\" \" + a for a in answers]\n",
    "\n",
    "    return prompts, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60fd23b8-0dc2-41a1-8e55-8f892ab1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = LanguageModel(\"gpt2-xl\", device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c769325-3c33-4ad0-9e16-6c0dda278cf3",
   "metadata": {},
   "source": [
    "## Running the Dataset\n",
    "\n",
    "Run a single prompt through the model. And observe its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675f0120-2b50-4a84-b794-78f8df013ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, answers = aggregate_prompts(df, 1, prepend_space=True)\n",
    "\n",
    "with model.forward(prompts) as invoker:\n",
    "    pass\n",
    "logits = invoker.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04367cfc-a1d9-47e4-a257-12a8619adaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test a prompt on a model and generate a textual representaiton of outputs\n",
    "def test_prompt(prompt: str, answer: str, model):\n",
    "    \n",
    "    with model.forward(prompt) as invoker:\n",
    "        pass\n",
    "\n",
    "    logits = invoker.output[0]\n",
    "    logits = logits[0,-1,:] # only over the final token\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    \n",
    "    sorted_indices = t.argsort(probs, descending=True)\n",
    "    val = model.tokenizer(answer).input_ids[0]\n",
    "    rank = (sorted_indices == val[0]).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    print(f\"[bold]Rank: {rank.item()}        Logit: {logits[val].item():.3f} Prob: {probs[val].item():.3f} Token: |{model.tokenizer.decode(val)}|[/]\")\n",
    "    \n",
    "    for i, (tok, prob) in enumerate(zip(probs.topk(10).indices, probs.topk(10).values)):\n",
    "        print(f\"Top {i}th token. Logit: {logits[tok]:.3f} Prob: {prob:.3f} Token: |{model.tokenizer.decode(tok)}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c801ac32-5df1-46cb-8024-8067d9c29b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.899</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.101</span><span style=\"font-weight: bold\"> Token: | France|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m8.899\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m0.101\u001b[0m\u001b[1m Token: | France|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 0th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.899</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.101</span> Token: | France|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 0th token. Logit: \u001b[1;36m8.899\u001b[0m Prob: \u001b[1;36m0.101\u001b[0m Token: | France|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 1th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.829</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.094</span> Token: | the|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 1th token. Logit: \u001b[1;36m8.829\u001b[0m Prob: \u001b[1;36m0.094\u001b[0m Token: | the|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 2th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.517</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.069</span> Token: | his|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 2th token. Logit: \u001b[1;36m8.517\u001b[0m Prob: \u001b[1;36m0.069\u001b[0m Token: | his|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 3th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.284</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.020</span> Token: | Haiti|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 3th token. Logit: \u001b[1;36m7.284\u001b[0m Prob: \u001b[1;36m0.020\u001b[0m Token: | Haiti|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 4th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.057</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016</span> Token: | Belgium|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 4th token. Logit: \u001b[1;36m7.057\u001b[0m Prob: \u001b[1;36m0.016\u001b[0m Token: | Belgium|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 5th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.876</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.013</span> Token: | Algeria|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 5th token. Logit: \u001b[1;36m6.876\u001b[0m Prob: \u001b[1;36m0.013\u001b[0m Token: | Algeria|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 6th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.721</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.011</span> Token: | Saint|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 6th token. Logit: \u001b[1;36m6.721\u001b[0m Prob: \u001b[1;36m0.011\u001b[0m Token: | Saint|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 7th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.543</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.010</span> Token: | origin|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 7th token. Logit: \u001b[1;36m6.543\u001b[0m Prob: \u001b[1;36m0.010\u001b[0m Token: | origin|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 8th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.478</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.009</span> Token: | Quebec|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 8th token. Logit: \u001b[1;36m6.478\u001b[0m Prob: \u001b[1;36m0.009\u001b[0m Token: | Quebec|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Top 9th token. Logit: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.280</span> Prob: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.007</span> Token: | its|\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Top 9th token. Logit: \u001b[1;36m6.280\u001b[0m Prob: \u001b[1;36m0.007\u001b[0m Token: | its|\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.tokenizer(answers).input_ids[0]\n",
    "test_prompt(prompts, answers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefefe5-63b0-426d-b53c-5ec562f39f7a",
   "metadata": {},
   "source": [
    "### Exercise: Calculate probabilities across a batch of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fa531bc-37c4-4ad1-baf3-dc60ee235c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "prompts, answers = aggregate_prompts(df, 200, prepend_space=True)\n",
    "\n",
    "with model.generate(max_new_tokens=1) as generator:\n",
    "    with generator.invoke(prompts) as invoker:\n",
    "        out = model.lm_head.output.t[-1].save()\n",
    "\n",
    "out = out.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0256e37f-c70b-4d4e-9594-b058f0e40b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2886, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = out.softmax(dim=-1)\n",
    "indices_tensor = t.tensor([model.tokenizer.encode(a) for a in answers]).to(device)\n",
    "top_probs = t.gather(probs, 1, indices_tensor)\n",
    "top_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "075b372f-fd2e-4fbf-89c9-318fbbda097d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Saint-Marcellin was created in the country of'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Law &amp; Order was released on'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Knowledge Graph is owned by'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Saint Valentine holds the position of the first'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'Francis Blanche speaks during a news conference at the'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Saint-Marcellin was created in the country of'\u001b[0m,\n",
       "    \u001b[32m'Law & Order was released on'\u001b[0m,\n",
       "    \u001b[32m'Knowledge Graph is owned by'\u001b[0m,\n",
       "    \u001b[32m'Saint Valentine holds the position of the first'\u001b[0m,\n",
       "    \u001b[32m'Francis Blanche speaks during a news conference at the'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Saint-Marcellin was created in the country of\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Saint-Marcellin was created in the country of\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m262\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m262\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4881</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4881\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4881</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4881\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts, answers = aggregate_prompts(df, 100, prepend_space=True)\n",
    "\n",
    "print(prompts[0:5])\n",
    "print(prompts[0])\n",
    "\n",
    "with model.forward(prompts) as invoker:\n",
    "    out_a = model.lm_head.output.t[-1].save()\n",
    "\n",
    "out_a = out_a.value\n",
    "out_b = invoker.output[0]\n",
    "\n",
    "print(new_out[0].argmax())\n",
    "print(out_b[0,-1,:].argmax())\n",
    "\n",
    "with model.forward(prompts[0]) as invoker:\n",
    "    out_c = model.lm_head.output.t[-1].save()\n",
    "\n",
    "out_c = out_c.value\n",
    "out_d = invoker.output[0]\n",
    "\n",
    "print(out_c[0].argmax())\n",
    "print(out_d[0,-1,:].argmax())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ed2fb-fc42-436a-9a09-58496e743729",
   "metadata": {},
   "source": [
    "# Module 2 (Activation Patching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27deb5-37e6-4487-b27c-07592cea0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First subject token\n",
    "# middle subject token\n",
    "# last subject token\n",
    "# first subsequent token\n",
    "# further tokens\n",
    "# last token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
