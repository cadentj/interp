{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"openai-community/gpt2-xl\", device_map=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_token = model.tokenizer.encode(\" Paris\")\n",
    "results = []\n",
    "\n",
    "def decoder(x):\n",
    "    return model.lm_head(model.transformer.ln_f(x))\n",
    "\n",
    "with model.trace(\"The capital of France is\"):\n",
    "    for layer in model.transformer.h:\n",
    "\n",
    "        logits = decoder(layer.output[0])\n",
    "        tokens = logits.softmax(-1)[:,-1,correct_token]\n",
    "        results.append(tokens.save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# results = [r.value.item() for r in results]\n",
    "\n",
    "plt.plot(results)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Layer')  # Replace with your desired label\n",
    "plt.ylabel('P( Paris)')  # Replace with your desired label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Space Needle is in downtown\"\n",
    "prompt = model.tokenizer.encode(prompt)\n",
    "\n",
    "correct_token = model.tokenizer.encode(\" Seattle\")\n",
    "\n",
    "n_tokens = len(prompt)\n",
    "n_layers = len(model.transformer.h)\n",
    "\n",
    "with model.trace(remote=False) as tracer:\n",
    "\n",
    "    clean_acts = []\n",
    "\n",
    "    with tracer.invoke(prompt):\n",
    "        for i, layer in enumerate(model.transformer.h):\n",
    "            clean_acts.append(layer.output[0])\n",
    "\n",
    "        probs = model.lm_head.output.softmax(-1)\n",
    "        clean_value = probs[:,-1, correct_token]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for t in range(n_tokens):\n",
    "\n",
    "        per_token_result = []\n",
    "        for layer in range(n_layers):\n",
    "            with tracer.invoke(prompt, scan=False):\n",
    "                # Corrupt the subject tokens\n",
    "                model.transformer.wte.output[:,0:3,:][:] = 0.\n",
    "\n",
    "                # Restore the clean activations\n",
    "                model.transformer.h[layer].output[0][:,t,:] = clean_acts[layer][:,t,:]\n",
    "                \n",
    "                probs = model.lm_head.output.softmax(-1)\n",
    "                difference = clean_value - probs[:,-1, correct_token]\n",
    "                \n",
    "                per_token_result.append(difference.item().save())                   \n",
    "\n",
    "        results.append(per_token_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results=  []\n",
    "\n",
    "for i in results:\n",
    "    row = []\n",
    "    for j in i:\n",
    "        temp = j.value\n",
    "        row.append(temp)\n",
    "    new_results.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "y_labels = [\"The*\", \"Space*\", \"Need*\", \"le*\", \"is\", \"in\", \"downtown\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "cax = ax.imshow(new_results, cmap='Purples_r', aspect='auto')\n",
    "\n",
    "# Set the y-axis labels\n",
    "ax.set_yticks(np.arange(len(y_labels)))\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "# Set the labels for the axes\n",
    "ax.set_xlabel('single restored layer within GPT-2-XL')\n",
    "\n",
    "cbar = fig.colorbar(cax, ax=ax, orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace() as tracer:\n",
    "\n",
    "    for i in range(12):\n",
    "        with tracer.invoke(\"hello\"):\n",
    "            if i == 11:\n",
    "                test = model.lm_head.output[:,-1,:].save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace(\"input\", remote=True):\n",
    "    output = model.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.trace() as tracer:\n",
    "    with tracer.invoke(\"Prompt One\"):\n",
    "        pass\n",
    "    with tracer.invoke(\"Prompt Two\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda:0\", dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((torch.Size([1, 1, 768]),), {})\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"hello\"):\n",
    "    print(model.transformer.h[0].mlp.input.shape)\n",
    "    print(model.transformer.h[0].mlp.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n_tokens = 10\n",
    "answer = \" Paris\"\n",
    "answer_token = model.tokenizer.encode(answer)[0]\n",
    "\n",
    "class LORA(nn.Module):\n",
    "    def __init__(self, module, dim, r: int) -> None:\n",
    "        super(LORA, self).__init__()\n",
    "        self.r = r\n",
    "        self.module = module\n",
    "\n",
    "        self.WA = torch.nn.Parameter(torch.randn(dim, self.r), requires_grad=True)\n",
    "        self.WB = torch.nn.Parameter(torch.zeros(self.r, dim), requires_grad=True)\n",
    "\n",
    "    def __call__(self, alpha:float=1.0):\n",
    "        A_x = torch.matmul(self.module.input[0][0], self.WA)\n",
    "        BA_x = torch.matmul(A_x, self.WB)\n",
    "        h = BA_x + self.module.output\n",
    "\n",
    "        self.module.output = h * alpha\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.WA, self.WB]\n",
    "\n",
    "lora = LORA(model.transformer.h[0].mlp, 768, 4).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [[\"_\" * n_tokens, answer_token]] * 100\n",
    "dataloader = DataLoader(dataset, batch_size=10)\n",
    "\n",
    "optimizer = torch.optim.AdamW(lora.parameters(), lr=.1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for i, (inputs, targets) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with model.trace(inputs) as runner:\n",
    "        lora()\n",
    "\n",
    "        logits = model.lm_head.output.save()\n",
    "\n",
    "    loss = loss_fn(logits[:, -1], targets.to(\"cuda:0\"))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__________ Paris']\n",
      "['__________\\n']\n"
     ]
    }
   ],
   "source": [
    "with model.generate(dataset[0][0]) as generator:\n",
    "    lora()\n",
    "\n",
    "    out = model.generator.output.save()\n",
    "\n",
    "\n",
    "\n",
    "print(model.tokenizer.batch_decode(out))\n",
    "\n",
    "with model.generate(dataset[0][0]) as generator:\n",
    "\n",
    "    out = model.generator.output.save()\n",
    "\n",
    "print(model.tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "name = \"meta-llama/Meta-Llama-3-70B\"\n",
    "model = LanguageModel(name)\n",
    "with model.trace(\"input\"):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
